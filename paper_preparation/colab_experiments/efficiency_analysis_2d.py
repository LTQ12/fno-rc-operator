"""
FNO-RC 2D Navier-Stokes è®¡ç®—æ•ˆç‡åˆ†æå®éªŒ
ä¸“ä¸ºGoogle Colabç¯å¢ƒè®¾è®¡ï¼Œèšç„¦æœ€æ˜¾è‘—æ”¹è¿›çš„ç»´åº¦
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import json
import os
import time
from datetime import datetime
import psutil
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥2Dæ¨¡å‹ç»„ä»¶ - ä¿®å¤å¯¼å…¥
import sys
sys.path.append('.')

from statistical_validation_2d_ns import (
    setup_colab_environment, load_navier_stokes_data, prepare_data_loaders_2d,
    LpLoss, StandardFNO2d, FNORCF2d
)

# ================================
# è®¡ç®—æ•ˆç‡æµ‹é‡å·¥å…·
# ================================

class EfficiencyProfiler2D:
    """2Dæ•ˆç‡åˆ†æå·¥å…·"""
    def __init__(self, device):
        self.device = device
        self.results = {}
    
    def profile_model_2d(self, model, data_loader, model_name, num_batches=20):
        """åˆ†æ2Dæ¨¡å‹çš„è®¡ç®—æ•ˆç‡"""
        model.eval()
        
        # å‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        # å†…å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            initial_memory = torch.cuda.memory_allocated()
        
        # æ¨ç†æ—¶é—´æµ‹è¯•
        inference_times = []
        
        with torch.no_grad():
            for i, (data, target) in enumerate(data_loader):
                if i >= num_batches:
                    break
                
                data = data.to(self.device)
                
                # é¢„çƒ­
                if i == 0:
                    _ = model(data)
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()
                
                # è®¡æ—¶
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                
                start_time = time.time()
                output = model(data)
                
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                
                end_time = time.time()
                
                if i > 0:  # è·³è¿‡ç¬¬ä¸€æ¬¡ï¼ˆé¢„çƒ­ï¼‰
                    inference_times.append(end_time - start_time)
        
        # å†…å­˜å³°å€¼
        if torch.cuda.is_available():
            peak_memory = torch.cuda.max_memory_allocated()
            memory_usage = peak_memory - initial_memory
        else:
            memory_usage = 0
            peak_memory = 0
        
        # FLOPsä¼°ç®—ï¼ˆ2Dç‰¹å®šï¼‰
        sample_input = next(iter(data_loader))[0][:1].to(self.device)
        flops = self.estimate_flops_2d(model, sample_input)
        
        results = {
            'total_parameters': int(total_params),
            'trainable_parameters': int(trainable_params),
            'inference_time_mean': float(np.mean(inference_times)),
            'inference_time_std': float(np.std(inference_times)),
            'memory_usage_mb': float(memory_usage / 1024 / 1024),
            'peak_memory_mb': float(peak_memory / 1024 / 1024),
            'estimated_flops': int(flops),
            'throughput_samples_per_sec': float(data_loader.batch_size / np.mean(inference_times)),
            'flops_per_sample': int(flops / data_loader.batch_size)
        }
        
        self.results[model_name] = results
        return results
    
    def estimate_flops_2d(self, model, sample_input):
        """2Dæ¨¡å‹çš„FLOPsä¼°ç®—"""
        total_params = sum(p.numel() for p in model.parameters())
        
        # è·å–è¾“å…¥å°ºå¯¸
        batch_size, h, w, channels = sample_input.shape
        
        # 2Då·ç§¯å’ŒFFTæ“ä½œçš„ç²—ç•¥ä¼°ç®—
        # FFT: O(N log N) where N = h * w
        fft_ops = batch_size * h * w * np.log2(h * w) * 2  # 2D FFT
        
        # çº¿æ€§å±‚æ“ä½œ
        linear_ops = total_params * batch_size * 2  # ä¹˜æ³•å’ŒåŠ æ³•
        
        # æ€»FLOPs
        total_flops = fft_ops + linear_ops
        
        return total_flops
    
    def measure_training_time(self, model, train_loader, device, epochs=10):
        """æµ‹é‡è®­ç»ƒæ—¶é—´"""
        model = model.to(device)
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        loss_fn = LpLoss(size_average=True)
        
        training_times = []
        
        for epoch in range(epochs):
            start_time = time.time()
            
            model.train()
            for batch_idx, (data, target) in enumerate(train_loader):
                data, target = data.to(device), target.to(device)
                
                optimizer.zero_grad()
                output = model(data)
                loss = loss_fn(output, target)
                loss.backward()
                optimizer.step()
                
                if batch_idx >= 10:  # åªè®­ç»ƒå‰10ä¸ªbatch
                    break
            
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            
            end_time = time.time()
            training_times.append(end_time - start_time)
            
            if epoch % 3 == 0:
                print(f'Training epoch {epoch}: {end_time - start_time:.2f}s')
        
        return {
            'mean_training_time_per_epoch': float(np.mean(training_times)),
            'std_training_time_per_epoch': float(np.std(training_times)),
            'total_training_time': float(np.sum(training_times))
        }
    
    def compare_models(self):
        """æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ•ˆç‡"""
        if len(self.results) < 2:
            return None
        
        comparison = {}
        baseline_name = list(self.results.keys())[0]
        baseline = self.results[baseline_name]
        
        for model_name, metrics in self.results.items():
            if model_name == baseline_name:
                continue
            
            comparison[model_name] = {
                'parameter_ratio': metrics['total_parameters'] / baseline['total_parameters'],
                'speed_ratio': baseline['inference_time_mean'] / metrics['inference_time_mean'],
                'memory_ratio': metrics['memory_usage_mb'] / max(baseline['memory_usage_mb'], 1),
                'flops_ratio': metrics['estimated_flops'] / baseline['estimated_flops'],
                'throughput_ratio': metrics['throughput_samples_per_sec'] / baseline['throughput_samples_per_sec']
            }
        
        return comparison

# ================================
# ä¸»å®éªŒå‡½æ•°
# ================================

def run_efficiency_experiments_2d():
    """è¿è¡Œ2Dè®¡ç®—æ•ˆç‡å®éªŒ"""
    device, base_path = setup_colab_environment()
    
    # åŠ è½½æ•°æ®
    train_a, train_u, test_a, test_u = load_navier_stokes_data()
    train_loader, test_loader = prepare_data_loaders_2d(train_a, train_u, test_a, test_u, batch_size=8)
    
    # åˆ›å»ºæ•ˆç‡åˆ†æå™¨
    profiler = EfficiencyProfiler2D(device)
    
    print("="*60)
    print("2D Navier-Stokes è®¡ç®—æ•ˆç‡åˆ†æå®éªŒ")
    print("="*60)
    
    # æµ‹è¯•æ¨¡å‹é…ç½®
    models_to_test = [
        {
            'name': 'Standard_FNO_2D',
            'model': StandardFNO2d(modes1=12, modes2=12, width=32, num_layers=4),
            'description': 'Baseline 2D FNO model'
        },
        {
            'name': 'FNO_RC_Small_2D',
            'model': FNORCF2d(modes1=12, modes2=12, width=32, num_layers=4, 
                             cft_segments=2, cft_modes1=4, cft_modes2=4),
            'description': 'FNO-RC with small CFT configuration'
        },
        {
            'name': 'FNO_RC_Standard_2D',
            'model': FNORCF2d(modes1=12, modes2=12, width=32, num_layers=4,
                             cft_segments=4, cft_modes1=8, cft_modes2=8),
            'description': 'FNO-RC with standard CFT configuration'
        },
        {
            'name': 'FNO_RC_Large_2D',
            'model': FNORCF2d(modes1=12, modes2=12, width=32, num_layers=4,
                             cft_segments=6, cft_modes1=12, cft_modes2=12),
            'description': 'FNO-RC with large CFT configuration'
        }
    ]
    
    efficiency_results = {}
    training_results = {}
    
    for model_config in models_to_test:
        print(f"\næµ‹è¯•æ¨¡å‹: {model_config['name']}")
        print(f"æè¿°: {model_config['description']}")
        print("-" * 40)
        
        model = model_config['model'].to(device)
        
        # æ¨ç†æ•ˆç‡åˆ†æ
        print("åˆ†ææ¨ç†æ•ˆç‡...")
        results = profiler.profile_model_2d(model, test_loader, model_config['name'])
        
        print(f"å‚æ•°é‡: {results['total_parameters']:,}")
        print(f"æ¨ç†æ—¶é—´: {results['inference_time_mean']:.4f} Â± {results['inference_time_std']:.4f} ç§’")
        print(f"å†…å­˜ä½¿ç”¨: {results['memory_usage_mb']:.1f} MB")
        print(f"ååé‡: {results['throughput_samples_per_sec']:.1f} æ ·æœ¬/ç§’")
        print(f"FLOPs: {results['estimated_flops']:,}")
        
        efficiency_results[model_config['name']] = results
        
        # è®­ç»ƒæ—¶é—´åˆ†æ
        print("åˆ†æè®­ç»ƒæ•ˆç‡...")
        training_time_results = profiler.measure_training_time(model, train_loader, device, epochs=5)
        training_results[model_config['name']] = training_time_results
        
        print(f"è®­ç»ƒæ—¶é—´/epoch: {training_time_results['mean_training_time_per_epoch']:.2f} Â± {training_time_results['std_training_time_per_epoch']:.2f} ç§’")
        
        # æ¸…ç†å†…å­˜
        del model
        torch.cuda.empty_cache()
    
    # æ¯”è¾ƒåˆ†æ
    comparison = profiler.compare_models()
    
    print("\n" + "="*60)
    print("2Dæ•ˆç‡å¯¹æ¯”åˆ†æ")
    print("="*60)
    
    if comparison:
        for model_name, ratios in comparison.items():
            print(f"\n{model_name} vs åŸºçº¿:")
            print(f"  å‚æ•°é‡å€æ•°: {ratios['parameter_ratio']:.2f}Ã—")
            print(f"  æ¨ç†é€Ÿåº¦æ¯”: {ratios['speed_ratio']:.2f}Ã— ({'æ›´å¿«' if ratios['speed_ratio'] > 1 else 'æ›´æ…¢'})")
            print(f"  å†…å­˜å€æ•°: {ratios['memory_ratio']:.2f}Ã—")
            print(f"  FLOPså€æ•°: {ratios['flops_ratio']:.2f}Ã—")
            print(f"  ååé‡æ¯”: {ratios['throughput_ratio']:.2f}Ã—")
    
    # ä¿å­˜æ•ˆç‡ç»“æœ
    efficiency_path = f"{base_path}/results/efficiency_analysis_2d"
    os.makedirs(efficiency_path, exist_ok=True)
    
    final_results = {
        'inference_efficiency': efficiency_results,
        'training_efficiency': training_results,
        'comparison': comparison,
        'metadata': {
            'problem': '2D Navier-Stokes',
            'device': str(device),
            'timestamp': datetime.now().isoformat(),
            'data_shape': f"train: {train_a.shape}, test: {test_a.shape}"
        }
    }
    
    with open(f"{efficiency_path}/2d_efficiency_results.json", 'w') as f:
        json.dump(final_results, f, indent=2)
    
    # ç”Ÿæˆå¯è§†åŒ–
    create_efficiency_plots_2d(efficiency_results, training_results, comparison, base_path)
    
    return final_results

def create_efficiency_plots_2d(efficiency_results, training_results, comparison, base_path):
    """åˆ›å»º2Dæ•ˆç‡åˆ†æå›¾è¡¨"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    model_names = list(efficiency_results.keys())
    
    # å­å›¾1: å‚æ•°é‡å¯¹æ¯”
    ax1 = axes[0, 0]
    params = [efficiency_results[name]['total_parameters'] for name in model_names]
    bars1 = ax1.bar(range(len(model_names)), params, alpha=0.7, color='skyblue')
    ax1.set_ylabel('å‚æ•°é‡')
    ax1.set_title('2D: æ¨¡å‹å‚æ•°é‡å¯¹æ¯”')
    ax1.set_xticks(range(len(model_names)))
    ax1.set_xticklabels([name.replace('_2D', '') for name in model_names], rotation=45)
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼
    for i, (bar, param) in enumerate(zip(bars1, params)):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{param/1000:.0f}K', ha='center', va='bottom', fontsize=8)
    
    # å­å›¾2: æ¨ç†æ—¶é—´å¯¹æ¯”
    ax2 = axes[0, 1]
    inference_times = [efficiency_results[name]['inference_time_mean'] for name in model_names]
    inference_stds = [efficiency_results[name]['inference_time_std'] for name in model_names]
    
    bars2 = ax2.bar(range(len(model_names)), inference_times, yerr=inference_stds, 
                   alpha=0.7, color='lightcoral', capsize=5)
    ax2.set_ylabel('æ¨ç†æ—¶é—´ (ç§’)')
    ax2.set_title('2D: æ¨ç†æ—¶é—´å¯¹æ¯”')
    ax2.set_xticks(range(len(model_names)))
    ax2.set_xticklabels([name.replace('_2D', '') for name in model_names], rotation=45)
    
    # å­å›¾3: å†…å­˜ä½¿ç”¨å¯¹æ¯”
    ax3 = axes[0, 2]
    memory_usage = [efficiency_results[name]['memory_usage_mb'] for name in model_names]
    bars3 = ax3.bar(range(len(model_names)), memory_usage, alpha=0.7, color='lightgreen')
    ax3.set_ylabel('å†…å­˜ä½¿ç”¨ (MB)')
    ax3.set_title('2D: å†…å­˜ä½¿ç”¨å¯¹æ¯”')
    ax3.set_xticks(range(len(model_names)))
    ax3.set_xticklabels([name.replace('_2D', '') for name in model_names], rotation=45)
    
    # å­å›¾4: ååé‡å¯¹æ¯”
    ax4 = axes[1, 0]
    throughput = [efficiency_results[name]['throughput_samples_per_sec'] for name in model_names]
    bars4 = ax4.bar(range(len(model_names)), throughput, alpha=0.7, color='gold')
    ax4.set_ylabel('ååé‡ (æ ·æœ¬/ç§’)')
    ax4.set_title('2D: ååé‡å¯¹æ¯”')
    ax4.set_xticks(range(len(model_names)))
    ax4.set_xticklabels([name.replace('_2D', '') for name in model_names], rotation=45)
    
    # å­å›¾5: FLOPså¯¹æ¯”
    ax5 = axes[1, 1]
    flops = [efficiency_results[name]['estimated_flops'] / 1e9 for name in model_names]  # è½¬æ¢ä¸ºGFLOPs
    bars5 = ax5.bar(range(len(model_names)), flops, alpha=0.7, color='mediumpurple')
    ax5.set_ylabel('FLOPs (GFLOPs)')
    ax5.set_title('2D: è®¡ç®—å¤æ‚åº¦å¯¹æ¯”')
    ax5.set_xticks(range(len(model_names)))
    ax5.set_xticklabels([name.replace('_2D', '') for name in model_names], rotation=45)
    
    # å­å›¾6: è®­ç»ƒæ—¶é—´å¯¹æ¯”
    ax6 = axes[1, 2]
    train_times = [training_results[name]['mean_training_time_per_epoch'] for name in model_names]
    train_stds = [training_results[name]['std_training_time_per_epoch'] for name in model_names]
    
    bars6 = ax6.bar(range(len(model_names)), train_times, yerr=train_stds,
                   alpha=0.7, color='lightsteelblue', capsize=5)
    ax6.set_ylabel('è®­ç»ƒæ—¶é—´/Epoch (ç§’)')
    ax6.set_title('2D: è®­ç»ƒæ—¶é—´å¯¹æ¯”')
    ax6.set_xticks(range(len(model_names)))
    ax6.set_xticklabels([name.replace('_2D', '') for name in model_names], rotation=45)
    
    plt.tight_layout()
    plt.savefig(f"{base_path}/results/efficiency_analysis_2d/2d_efficiency_comparison.png",
                dpi=300, bbox_inches='tight')
    plt.show()
    
    # åˆ›å»ºæ•ˆç‡æ¯”è¾ƒé›·è¾¾å›¾
    create_efficiency_radar_chart(efficiency_results, comparison, base_path)

def create_efficiency_radar_chart(efficiency_results, comparison, base_path):
    """åˆ›å»ºæ•ˆç‡æ¯”è¾ƒé›·è¾¾å›¾"""
    if not comparison:
        return
    
    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
    
    # å®šä¹‰æŒ‡æ ‡
    metrics = ['å‚æ•°æ•ˆç‡', 'æ¨ç†é€Ÿåº¦', 'å†…å­˜æ•ˆç‡', 'è®¡ç®—æ•ˆç‡', 'ååé‡']
    
    # è§’åº¦
    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆåœ†åœˆ
    
    # ä¸ºæ¯ä¸ªæ¨¡å‹ç»˜åˆ¶é›·è¾¾å›¾
    colors = ['blue', 'green', 'orange']
    model_names = list(comparison.keys())
    
    for i, (model_name, ratios) in enumerate(comparison.items()):
        # è®¡ç®—æ•ˆç‡å¾—åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        values = [
            1 / ratios['parameter_ratio'],  # å‚æ•°æ•ˆç‡ï¼šå‚æ•°è¶Šå°‘è¶Šå¥½
            ratios['speed_ratio'],          # æ¨ç†é€Ÿåº¦ï¼šè¶Šå¿«è¶Šå¥½
            1 / ratios['memory_ratio'],     # å†…å­˜æ•ˆç‡ï¼šå†…å­˜è¶Šå°‘è¶Šå¥½
            1 / ratios['flops_ratio'],      # è®¡ç®—æ•ˆç‡ï¼šFLOPsè¶Šå°‘è¶Šå¥½
            ratios['throughput_ratio']      # ååé‡ï¼šè¶Šé«˜è¶Šå¥½
        ]
        values += values[:1]  # é—­åˆåœ†åœˆ
        
        ax.plot(angles, values, 'o-', linewidth=2, label=model_name.replace('_2D', ''), color=colors[i % len(colors)])
        ax.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])
    
    # æ·»åŠ æ ‡ç­¾
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(metrics)
    ax.set_ylim(0, 2)
    ax.set_title('2Dæ¨¡å‹æ•ˆç‡å¯¹æ¯”é›·è¾¾å›¾\n(æ•°å€¼è¶Šå¤§è¡¨ç¤ºæ•ˆç‡è¶Šé«˜)', pad=20)
    ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
    ax.grid(True)
    
    plt.savefig(f"{base_path}/results/efficiency_analysis_2d/2d_efficiency_radar.png",
                dpi=300, bbox_inches='tight')
    plt.show()

# ================================
# ä¸»æ‰§è¡Œå‡½æ•°
# ================================

if __name__ == "__main__":
    print("FNO-RC 2D Navier-Stokes è®¡ç®—æ•ˆç‡åˆ†æå®éªŒ")
    print("é€‚ç”¨äºGoogle Colabç¯å¢ƒ")
    print("ä¸“æ³¨äº73.68%æ”¹è¿›çš„æœ€æ˜¾è‘—ç»“æœ")
    print("é¢„è®¡è¿è¡Œæ—¶é—´: 1-2å°æ—¶")
    
    # è¿è¡Œå®éªŒ
    results = run_efficiency_experiments_2d()
    
    print("\nğŸ‰ 2D Navier-Stokesæ•ˆç‡åˆ†æå®éªŒå®Œæˆï¼")
    print("å·²è¯¦ç»†åˆ†æ73.68%æ”¹è¿›çš„è®¡ç®—å¼€é”€ã€‚")
    print("ç»“æœå·²ä¿å­˜åˆ°Google Driveã€‚")
