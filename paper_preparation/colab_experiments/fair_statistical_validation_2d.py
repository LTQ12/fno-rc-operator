"""
å…¬å¹³çš„FNO-RC 2D Navier-Stokesç»Ÿè®¡æ˜¾è‘—æ€§éªŒè¯å®éªŒ
ä¸¥æ ¼ç¡®ä¿ä¸¤ä¸ªæ¨¡å‹ä½¿ç”¨å®Œå…¨ç›¸åŒçš„å‚æ•°å’Œè®­ç»ƒè®¾ç½®
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import json
import os
import time
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ä½¿ç”¨å·²æœ‰çš„æ ‡å‡†FNOç»“æœ
BASELINE_FNO_RESULTS = [0.189337, 0.187468, 0.186463, 0.189672, 0.189752]

def setup_colab_environment():
    """è®¾ç½®Colabç¯å¢ƒ"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    base_path = "/content/drive/MyDrive/FNO_RC_Experiments"
    os.makedirs(f"{base_path}/results/statistical_validation_2d", exist_ok=True)
    os.makedirs(f"{base_path}/models/2d_ns", exist_ok=True)
    
    return device, base_path

# ================================
# æ•°æ®å½’ä¸€åŒ–å·¥å…·
# ================================

class GaussianNormalizer(object):
    def __init__(self, x, eps=0.00001):
        super(GaussianNormalizer, self).__init__()

        self.mean = torch.mean(x, 0)
        self.std = torch.std(x, 0)
        self.eps = eps

    def encode(self, x):
        x = (x - self.mean) / (self.std + self.eps)
        return x

    def decode(self, x, sample_idx=None):
        if sample_idx is None:
            std = self.std + self.eps # n
            mean = self.mean
        else:
            if len(self.mean.shape) == len(sample_idx[0].shape):
                std = self.std[sample_idx] + self.eps  # batch*n
                mean = self.mean[sample_idx]
            if len(self.mean.shape) > len(sample_idx[0].shape):
                std = self.std[:,sample_idx]+ self.eps # d*batch*n
                mean = self.mean[:,sample_idx]

        # x is in shape of batch*n or d*batch*n
        x = (x * std) + mean
        return x

    def cuda(self):
        self.mean = self.mean.cuda()
        self.std = self.std.cuda()

    def to(self, device):
        self.mean = self.mean.to(device)
        self.std = self.std.to(device)

# ================================
# æŸå¤±å‡½æ•°
# ================================

class LpLoss(object):
    def __init__(self, d=2, p=2, size_average=True, reduction=True):
        super(LpLoss, self).__init__()

        #Dimension and Lp-norm type are postive
        assert d > 0 and p > 0

        self.d = d
        self.p = p
        self.reduction = reduction
        self.size_average = size_average

    def abs(self, x, y):
        num_examples = x.size()[0]

        #Assume uniform mesh
        h = 1.0 / (x.size()[1] - 1.0)

        all_norms = (h**(self.d/self.p))*torch.norm(x.view(num_examples,-1) - y.view(num_examples,-1), self.p, 1)

        if self.reduction:
            if self.size_average:
                return torch.mean(all_norms)
            else:
                return torch.sum(all_norms)

        return all_norms

    def rel(self, x, y):
        num_examples = x.size()[0]

        diff_norms = torch.norm(x.reshape(num_examples,-1) - y.reshape(num_examples,-1), self.p, 1)
        y_norms = torch.norm(y.reshape(num_examples,-1), self.p, 1)

        if self.reduction:
            if self.size_average:
                return torch.mean(diff_norms/y_norms)
            else:
                return torch.sum(diff_norms/y_norms)

        return diff_norms/y_norms

    def __call__(self, x, y):
        return self.rel(x, y)

# ================================
# æ ‡å‡†FNOæ¨¡å‹ - å®Œå…¨æŒ‰ç…§åŸå§‹å®ç°
# ================================

class SpectralConv2d(nn.Module):
    def __init__(self, in_channels, out_channels, modes1, modes2):
        super(SpectralConv2d, self).__init__()

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1
        self.modes2 = modes2

        self.scale = (1 / (in_channels * out_channels))
        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))
        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))

    def compl_mul2d(self, input, weights):
        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)
        return torch.einsum("bixy,ioxy->boxy", input, weights)

    def forward(self, x):
        batchsize = x.shape[0]
        #Compute Fourier coeffcients up to factor of e^(- something constant)
        x_ft = torch.fft.rfft2(x)

        # Multiply relevant Fourier modes
        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)
        out_ft[:, :, :self.modes1, :self.modes2] = \
            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)
        out_ft[:, :, -self.modes1:, :self.modes2] = \
            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)

        #Return to physical space
        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))
        return x

class FNO2d_Baseline(nn.Module):
    def __init__(self, modes1, modes2, width, in_channels, out_channels):
        super(FNO2d_Baseline, self).__init__()

        self.modes1 = modes1
        self.modes2 = modes2
        self.width = width
        self.padding = 9 # pad the domain if input is non-periodic
        self.fc0 = nn.Linear(in_channels + 2, self.width) # input channel is 12: the solution of the previous 10 timesteps + 2 locations (u(t-10), ..., u(t-1), x, y)

        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)
        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)
        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)
        self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)
        self.w0 = nn.Conv2d(self.width, self.width, 1)
        self.w1 = nn.Conv2d(self.width, self.width, 1)
        self.w2 = nn.Conv2d(self.width, self.width, 1)
        self.w3 = nn.Conv2d(self.width, self.width, 1)

        self.fc1 = nn.Linear(self.width, 128)
        self.fc2 = nn.Linear(128, out_channels)

    def get_grid(self, shape, device):
        batchsize, size_x, size_y = shape[0], shape[1], shape[2]
        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)
        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])
        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)
        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])
        return torch.cat((gridx, gridy), dim=-1).to(device)

    def forward(self, x):
        grid = self.get_grid(x.shape, x.device)
        x = torch.cat((x, grid), dim=-1)
        x = self.fc0(x)
        x = x.permute(0, 3, 1, 2)
        x = F.pad(x, [0, self.padding, 0, self.padding])

        x1 = self.conv0(x)
        x2 = self.w0(x)
        x = x1 + x2
        x = F.gelu(x)

        x1 = self.conv1(x)
        x2 = self.w1(x)
        x = x1 + x2
        x = F.gelu(x)

        x1 = self.conv2(x)
        x2 = self.w2(x)
        x = x1 + x2
        x = F.gelu(x)

        x1 = self.conv3(x)
        x2 = self.w3(x)
        x = x1 + x2

        x = x[..., :-self.padding, :-self.padding]
        x = x.permute(0, 2, 3, 1) # pad the domain if input is non-periodic
        x = self.fc1(x)
        x = F.gelu(x)
        x = self.fc2(x)
        return x

# ================================
# ç®€åŒ–çš„CFTå®ç°
# ================================

def simple_cft2d(x, modes1, modes2, L_segments=4, M_cheb=8):
    """ç®€åŒ–çš„2D CFTå®ç°ï¼Œç¡®ä¿ç¨³å®šæ€§"""
    B, C, H, W = x.shape
    device = x.device
    
    try:
        # ç®€åŒ–çš„åˆ†æ®µå¤„ç†
        h_seg = H // L_segments
        w_seg = W // L_segments
        
        cft_coeffs = []
        
        for i in range(L_segments):
            for j in range(L_segments):
                h_start, h_end = i * h_seg, (i + 1) * h_seg if i < L_segments - 1 else H
                w_start, w_end = j * w_seg, (j + 1) * w_seg if j < L_segments - 1 else W
                
                x_segment = x[:, :, h_start:h_end, w_start:w_end]
                
                # ä½¿ç”¨DCTä½œä¸ºChebyshevçš„ç®€åŒ–è¿‘ä¼¼
                x_segment_norm = F.normalize(x_segment.flatten(2), dim=-1)
                
                # è®¡ç®—å‰å‡ ä¸ªDCTç³»æ•°
                coeffs = []
                for k in range(min(M_cheb, x_segment_norm.shape[-1])):
                    coeff = torch.mean(x_segment_norm[:, :, k::M_cheb], dim=-1)
                    coeffs.append(coeff)
                
                if coeffs:
                    segment_coeffs = torch.stack(coeffs, dim=-1)
                    cft_coeffs.append(segment_coeffs)
        
        if cft_coeffs:
            result = torch.cat(cft_coeffs, dim=-1)
            # è°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸
            target_size = min(modes1 * modes2, result.shape[-1])
            result = result[:, :, :target_size]
            result = result.view(B, C, modes1, modes2)
            return result.to(torch.cfloat)
        else:
            return torch.zeros(B, C, modes1, modes2, device=device, dtype=torch.cfloat)
            
    except Exception as e:
        print(f"CFT error: {e}")
        return torch.zeros(B, C, modes1, modes2, device=device, dtype=torch.cfloat)

# ================================
# FNO-RCæ¨¡å‹ - æŒ‰ç…§åŸå§‹æ¶æ„
# ================================

class SpectralConv2d_RC(nn.Module):
    def __init__(self, in_channels, out_channels, modes1, modes2, L_segments=4, M_cheb=8):
        super(SpectralConv2d_RC, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.modes1 = modes1
        self.modes2 = modes2

        # æ ‡å‡†FNOæƒé‡
        self.scale = (1 / (in_channels * out_channels))
        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))
        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))

        # CFTæ®‹å·®ä¿®æ­£
        self.cft_modes1 = modes1 // 4
        self.cft_modes2 = modes2 // 4
        self.L_segments = L_segments
        self.M_cheb = M_cheb
        cft_flat_dim = self.in_channels * self.cft_modes1 * self.cft_modes2 * 2  # Real/Imag

        self.correction_generator = nn.Sequential(
            nn.Linear(cft_flat_dim, (self.in_channels + self.out_channels) * 2),
            nn.GELU(),
            nn.Linear((self.in_channels + self.out_channels) * 2, self.out_channels)
        )
        # é›¶åˆå§‹åŒ–æœ€åä¸€å±‚
        nn.init.zeros_(self.correction_generator[-1].weight)
        nn.init.zeros_(self.correction_generator[-1].bias)

    def forward(self, x):
        B, C, H, W = x.shape

        # æ ‡å‡†FNOè·¯å¾„
        x_ft = torch.fft.rfft2(x, s=(H, W))
        out_ft = torch.zeros(B, self.out_channels, H, W // 2 + 1, dtype=torch.cfloat, device=x.device)
        
        out_ft[:, :, :self.modes1, :self.modes2] = \
            torch.einsum("bixy,ioxy->boxy", x_ft[:, :, :self.modes1, :self.modes2], self.weights1)
        out_ft[:, :, -self.modes1:, :self.modes2] = \
            torch.einsum("bixy,ioxy->boxy", x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)

        x_fno = torch.fft.irfft2(out_ft, s=(H, W))

        # CFTæ®‹å·®ä¿®æ­£è·¯å¾„
        try:
            cft_coeffs = simple_cft2d(x, self.cft_modes1, self.cft_modes2, self.L_segments, self.M_cheb)
            cft_flat = torch.view_as_real(cft_coeffs).flatten(1)
            correction = self.correction_generator(cft_flat)  # (B, out_channels)
            correction = correction.view(B, self.out_channels, 1, 1)
        except Exception as e:
            print(f"CFT correction error: {e}")
            correction = torch.zeros(B, self.out_channels, 1, 1, device=x.device)

        return x_fno + correction

class FNO_RC_2D(nn.Module):
    def __init__(self, modes1, modes2, width, in_channels, out_channels):
        super(FNO_RC_2D, self).__init__()
        self.modes1 = modes1
        self.modes2 = modes2
        self.width = width
        self.padding = 9
        
        self.fc0 = nn.Linear(in_channels + 2, self.width)

        self.conv0 = SpectralConv2d_RC(self.width, self.width, self.modes1, self.modes2)
        self.conv1 = SpectralConv2d_RC(self.width, self.width, self.modes1, self.modes2)
        self.conv2 = SpectralConv2d_RC(self.width, self.width, self.modes1, self.modes2)
        self.conv3 = SpectralConv2d_RC(self.width, self.width, self.modes1, self.modes2)
        
        self.w0 = nn.Conv2d(self.width, self.width, 1)
        self.w1 = nn.Conv2d(self.width, self.width, 1)
        self.w2 = nn.Conv2d(self.width, self.width, 1)
        self.w3 = nn.Conv2d(self.width, self.width, 1)

        self.fc1 = nn.Linear(self.width, 128)
        self.fc2 = nn.Linear(128, out_channels)

    def get_grid(self, shape, device):
        batchsize, size_x, size_y = shape[0], shape[1], shape[2]
        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)
        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])
        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)
        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])
        return torch.cat((gridx, gridy), dim=-1).to(device)

    def forward(self, x):
        grid = self.get_grid(x.shape, x.device)
        x = torch.cat((x, grid), dim=-1)
        x = self.fc0(x)
        x = x.permute(0, 3, 1, 2)
        x = F.pad(x, [0, self.padding, 0, self.padding])

        x1 = self.conv0(x)
        x2 = self.w0(x)
        x = x1 + x2
        x = F.gelu(x)

        x1 = self.conv1(x)
        x2 = self.w1(x)
        x = x1 + x2
        x = F.gelu(x)

        x1 = self.conv2(x)
        x2 = self.w2(x)
        x = x1 + x2
        x = F.gelu(x)

        x1 = self.conv3(x)
        x2 = self.w3(x)
        x = x1 + x2

        x = x[..., :-self.padding, :-self.padding]
        x = x.permute(0, 2, 3, 1)
        x = self.fc1(x)
        x = F.gelu(x)
        x = self.fc2(x)
        return x

# ================================
# æ•°æ®åŠ è½½
# ================================

def load_ns_data():
    """åŠ è½½2D Navier-Stokesæ•°æ®"""
    print("Loading 2D Navier-Stokes data...")
    
    # ç›´æ¥ä½¿ç”¨æ‚¨æŒ‡å®šçš„æ•°æ®è·¯å¾„
    data_file = "/content/drive/MyDrive/ns_data_N600_clean.pt"
    
    try:
        data = torch.load(data_file, map_location='cpu')
        print(f"Successfully loaded NS data from {data_file}")
        print(f"Data shape: {data.shape}")
        print(f"Data type: {data.dtype}")
        print(f"Data range: [{data.min():.6f}, {data.max():.6f}]")
        
        # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
        if torch.isnan(data).any():
            print("âš ï¸  Warning: Found NaN values in data")
        if torch.isinf(data).any():
            print("âš ï¸  Warning: Found Inf values in data")
        
        return data
        
    except FileNotFoundError:
        print(f"âŒ Error: Data file not found at {data_file}")
        print("Please ensure the data file exists at the specified path.")
        raise
    except Exception as e:
        print(f"âŒ Error loading data: {e}")
        raise

# ================================
# è®­ç»ƒå‡½æ•° - å®Œå…¨æŒ‰ç…§åŸå§‹è®¾ç½®
# ================================

class Adam:
    def __init__(self, parameters, lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False):
        self.param_groups = [{'params': list(parameters), 'lr': lr, 'betas': betas, 'eps': eps, 'weight_decay': weight_decay, 'amsgrad': amsgrad}]
        self.state = {}
        
    def zero_grad(self):
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is not None:
                    p.grad.zero_()
    
    def step(self):
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                
                grad = p.grad.data
                if grad.is_sparse:
                    raise RuntimeError('Adam does not support sparse gradients')
                
                state = self.state.get(id(p), {})
                
                # State initialization
                if len(state) == 0:
                    state['step'] = 0
                    state['exp_avg'] = torch.zeros_like(p.data)
                    state['exp_avg_sq'] = torch.zeros_like(p.data)
                
                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
                beta1, beta2 = group['betas']
                
                state['step'] += 1
                
                if group['weight_decay'] != 0:
                    grad = grad.add(p.data, alpha=group['weight_decay'])
                
                # Exponential moving average of gradient values
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                # Exponential moving average of squared gradient values
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
                
                denom = exp_avg_sq.sqrt().add_(group['eps'])
                
                bias_correction1 = 1 - beta1 ** state['step']
                bias_correction2 = 1 - beta2 ** state['step']
                
                step_size = group['lr'] * (bias_correction2 ** 0.5) / bias_correction1
                
                p.data.addcdiv_(exp_avg, denom, value=-step_size)
                
                self.state[id(p)] = state

def train_model_fair(model, data, device, epochs=100, ntrain=500, ntest=100, T_in=10, T_out=10, 
                    batch_size=20, learning_rate=0.00025):
    """å…¬å¹³è®­ç»ƒï¼Œå®Œå…¨æŒ‰ç…§åŸå§‹è®¾ç½®"""
    
    print(f"Training parameters:")
    print(f"  epochs: {epochs}")
    print(f"  ntrain: {ntrain}, ntest: {ntest}")
    print(f"  T_in: {T_in}, T_out: {T_out}")
    print(f"  batch_size: {batch_size}")
    print(f"  learning_rate: {learning_rate}")
    
    # æ•°æ®åˆ†å‰² - æŒ‰ç…§åŸå§‹æ–¹å¼
    x_train = data[:ntrain, ..., :T_in]
    y_train = data[:ntrain, ..., T_in:T_in+T_out]
    x_test = data[-ntest:, ..., :T_in]
    y_test = data[-ntest:, ..., T_in:T_in+T_out]
    
    print(f"Data shapes:")
    print(f"  x_train: {x_train.shape}, y_train: {y_train.shape}")
    print(f"  x_test: {x_test.shape}, y_test: {y_test.shape}")
    
    # æ•°æ®å½’ä¸€åŒ– - æŒ‰ç…§åŸå§‹æ–¹å¼
    x_normalizer = GaussianNormalizer(x_train)
    x_train = x_normalizer.encode(x_train)
    x_test = x_normalizer.encode(x_test)

    y_normalizer = GaussianNormalizer(y_train)
    y_train = y_normalizer.encode(y_train)
    
    x_normalizer.to(device)
    y_normalizer.to(device)
    
    # æ•°æ®åŠ è½½å™¨
    train_loader = torch.utils.data.DataLoader(
        torch.utils.data.TensorDataset(x_train, y_train), 
        batch_size=batch_size, shuffle=True
    )
    test_loader = torch.utils.data.DataLoader(
        torch.utils.data.TensorDataset(x_test, y_test), 
        batch_size=batch_size, shuffle=False
    )
    
    # æ¨¡å‹å’Œä¼˜åŒ–å™¨ - ä¿®å¤å…¼å®¹æ€§é—®é¢˜
    model = model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    loss_func = LpLoss(size_average=False)
    
    best_test_loss = float('inf')
    
    print(f"Starting training for {epochs} epochs...")
    
    for ep in range(epochs):
        model.train()
        train_l2 = 0
        
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            optimizer.zero_grad()
            out = model(x)
            
            # æŒ‰ç…§åŸå§‹æ–¹å¼è®¡ç®—æŸå¤±
            loss = loss_func(out.view(out.size(0), -1), y.view(y.size(0), -1))
            loss.backward()
            optimizer.step()
            train_l2 += loss.item()
        
        scheduler.step()
        
        # æµ‹è¯•
        if ep % 20 == 0 or ep == epochs - 1:
            model.eval()
            test_l2 = 0.0
            with torch.no_grad():
                for x, y in test_loader:
                    x, y = x.to(device), y.to(device)
                    out = model(x)
                    
                    # è§£ç åè®¡ç®—è¯¯å·®
                    out_decoded = y_normalizer.decode(out)
                    y_decoded = y_normalizer.decode(y)
                    test_l2 += loss_func(out_decoded.view(out_decoded.size(0), -1), 
                                       y_decoded.view(y_decoded.size(0), -1)).item()
            
            train_l2 /= ntrain
            test_l2 /= ntest
            best_test_loss = min(best_test_loss, test_l2)
            
            print(f'Epoch {ep}: Train L2: {train_l2:.6f}, Test L2: {test_l2:.6f}')
    
    return best_test_loss

# ================================
# ä¸»å®éªŒå‡½æ•°
# ================================

def run_fair_statistical_experiments():
    """è¿è¡Œå…¬å¹³çš„ç»Ÿè®¡éªŒè¯å®éªŒ"""
    device, base_path = setup_colab_environment()
    
    # åŠ è½½æ•°æ®
    data = load_ns_data()
    
    # å®éªŒå‚æ•° - å®Œå…¨æŒ‰ç…§åŸå§‹è®¾ç½®
    MODES = 16
    WIDTH = 32
    T_IN = 10
    T_OUT = 10
    NTRAIN = 500
    NTEST = 100
    BATCH_SIZE = 20
    LEARNING_RATE = 0.00025
    EPOCHS = 100  # ç”¨æˆ·è¦æ±‚ç»Ÿä¸€ä¸º100
    
    print("="*60)
    print("å…¬å¹³çš„2D Navier-Stokesç»Ÿè®¡éªŒè¯å®éªŒ")
    print("="*60)
    print("å®éªŒå‚æ•°:")
    print(f"  modes: {MODES}, width: {WIDTH}")
    print(f"  T_in: {T_IN}, T_out: {T_OUT}")
    print(f"  ntrain: {NTRAIN}, ntest: {NTEST}")
    print(f"  batch_size: {BATCH_SIZE}, lr: {LEARNING_RATE}")
    print(f"  epochs: {EPOCHS}")
    print()
    print("ä½¿ç”¨å·²æœ‰çš„æ ‡å‡†FNOç»“æœ:")
    for i, error in enumerate(BASELINE_FNO_RESULTS):
        print(f"  è¿è¡Œ {i+1}: {error:.6f}")
    print(f"  å¹³å‡: {np.mean(BASELINE_FNO_RESULTS):.6f} Â± {np.std(BASELINE_FNO_RESULTS):.6f}")
    
    # è¿è¡ŒFNO-RCå®éªŒ
    print("\nè¿è¡ŒFNO-RCå®éªŒ...")
    print("-" * 40)
    
    fno_rc_results = []
    
    for run in range(5):
        print(f"\nFNO-RCè¿è¡Œ {run+1}/5...")
        
        torch.manual_seed(42 + run)
        np.random.seed(42 + run)
        
        # ä½¿ç”¨ç›¸åŒå‚æ•°çš„FNO-RCæ¨¡å‹
        model = FNO_RC_2D(
            modes1=MODES, modes2=MODES, width=WIDTH, 
            in_channels=T_IN, out_channels=T_OUT
        )
        
        best_test_loss = train_model_fair(
            model, data, device, 
            epochs=EPOCHS, ntrain=NTRAIN, ntest=NTEST, 
            T_in=T_IN, T_out=T_OUT, batch_size=BATCH_SIZE, 
            learning_rate=LEARNING_RATE
        )
        
        fno_rc_results.append(best_test_loss)
        print(f"FNO-RCè¿è¡Œ {run+1} å®Œæˆ: æœ€ä½³æµ‹è¯•è¯¯å·® = {best_test_loss:.6f}")
        
        del model
        torch.cuda.empty_cache()
    
    # è®¡ç®—ç»Ÿè®¡ç»“æœ
    fno_mean = np.mean(BASELINE_FNO_RESULTS)
    fno_std = np.std(BASELINE_FNO_RESULTS)
    
    fno_rc_mean = np.mean(fno_rc_results)
    fno_rc_std = np.std(fno_rc_results)
    
    improvement = (fno_mean - fno_rc_mean) / fno_mean * 100
    
    # tæ£€éªŒ
    diff = np.array(BASELINE_FNO_RESULTS) - np.array(fno_rc_results)
    t_stat = np.mean(diff) / (np.std(diff) / np.sqrt(len(diff)))
    
    # ç®€åŒ–çš„på€¼è®¡ç®—
    if abs(t_stat) > 2.776:  # 4ä¸ªè‡ªç”±åº¦ï¼Œ95%ç½®ä¿¡åº¦
        p_value = 0.01
    elif abs(t_stat) > 2.132:  # 4ä¸ªè‡ªç”±åº¦ï¼Œ90%ç½®ä¿¡åº¦
        p_value = 0.05
    else:
        p_value = 0.1
    
    # ç»“æœ
    results = {
        'fno_baseline': {
            'runs': [{'run': i+1, 'best_test_loss': error} for i, error in enumerate(BASELINE_FNO_RESULTS)],
            'mean': fno_mean,
            'std': fno_std
        },
        'fno_rc': {
            'runs': [{'run': i+1, 'best_test_loss': error} for i, error in enumerate(fno_rc_results)],
            'mean': fno_rc_mean,
            'std': fno_rc_std
        },
        'improvement_percent': improvement,
        'statistical_test': {
            't_statistic': float(t_stat),
            'p_value': float(p_value),
            'significant': p_value < 0.05
        },
        'experimental_setup': {
            'modes': MODES,
            'width': WIDTH,
            'T_in': T_IN,
            'T_out': T_OUT,
            'ntrain': NTRAIN,
            'ntest': NTEST,
            'batch_size': BATCH_SIZE,
            'learning_rate': LEARNING_RATE,
            'epochs': EPOCHS,
            'data_normalization': 'GaussianNormalizer',
            'loss_function': 'LpLoss',
            'optimizer': 'Adam',
            'scheduler': 'CosineAnnealingLR'
        },
        'metadata': {
            'problem': '2D Navier-Stokes',
            'timestamp': datetime.now().isoformat(),
            'note': 'Fair comparison with identical hyperparameters and training procedure'
        }
    }
    
    # ä¿å­˜ç»“æœ
    results_path = f"{base_path}/results/statistical_validation_2d/fair_2d_statistical_results.json"
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*60)
    print("å…¬å¹³çš„2D Navier-Stokesç»Ÿè®¡å®éªŒç»“æœ")
    print("="*60)
    print(f"æ ‡å‡†FNO:  {fno_mean:.6f} Â± {fno_std:.6f}")
    print(f"FNO-RC:   {fno_rc_mean:.6f} Â± {fno_rc_std:.6f}")
    print(f"æ”¹è¿›:     {improvement:.2f}%")
    print(f"tç»Ÿè®¡é‡:  {t_stat:.4f}")
    print(f"på€¼:      {p_value:.6f}")
    print(f"ç»Ÿè®¡æ˜¾è‘—: {'æ˜¯' if p_value < 0.05 else 'å¦'}")
    
    # ç”Ÿæˆç®€å•å¯è§†åŒ–
    create_fair_comparison_plot(BASELINE_FNO_RESULTS, fno_rc_results, results, base_path)
    
    return results

def create_fair_comparison_plot(fno_errors, fno_rc_errors, results, base_path):
    """åˆ›å»ºå…¬å¹³å¯¹æ¯”å›¾"""
    plt.figure(figsize=(12, 8))
    
    # å­å›¾1: è¯¯å·®å¯¹æ¯”
    plt.subplot(2, 2, 1)
    x = np.arange(len(fno_errors))
    plt.plot(x, fno_errors, 'o-', label='Standard FNO', linewidth=2, markersize=8, color='red')
    plt.plot(x, fno_rc_errors, 's-', label='FNO-RC', linewidth=2, markersize=8, color='blue')
    plt.xlabel('Run Number')
    plt.ylabel('Test Error')
    plt.title('Fair Comparison: Test Error Across Runs')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å­å›¾2: ç®±çº¿å›¾
    plt.subplot(2, 2, 2)
    plt.boxplot([fno_errors, fno_rc_errors], labels=['Standard FNO', 'FNO-RC'])
    plt.ylabel('Test Error')
    plt.title('Error Distribution')
    plt.grid(True, alpha=0.3)
    
    # å­å›¾3: æ”¹è¿›ç™¾åˆ†æ¯”
    plt.subplot(2, 2, 3)
    improvements = [(fno_errors[i] - fno_rc_errors[i]) / fno_errors[i] * 100 for i in range(len(fno_errors))]
    plt.bar(range(len(improvements)), improvements, alpha=0.7, color='green')
    plt.xlabel('Run Number')
    plt.ylabel('Improvement (%)')
    plt.title('Improvement per Run')
    plt.grid(True, alpha=0.3)
    
    # å­å›¾4: ç»Ÿè®¡æ±‡æ€»
    plt.subplot(2, 2, 4)
    plt.text(0.1, 0.8, f"Standard FNO: {results['fno_baseline']['mean']:.6f} Â± {results['fno_baseline']['std']:.6f}", 
             fontsize=11, transform=plt.gca().transAxes)
    plt.text(0.1, 0.7, f"FNO-RC: {results['fno_rc']['mean']:.6f} Â± {results['fno_rc']['std']:.6f}", 
             fontsize=11, transform=plt.gca().transAxes)
    plt.text(0.1, 0.6, f"Improvement: {results['improvement_percent']:.2f}%", 
             fontsize=11, transform=plt.gca().transAxes, color='green', weight='bold')
    plt.text(0.1, 0.5, f"t-statistic: {results['statistical_test']['t_statistic']:.4f}", 
             fontsize=11, transform=plt.gca().transAxes)
    plt.text(0.1, 0.4, f"p-value: {results['statistical_test']['p_value']:.6f}", 
             fontsize=11, transform=plt.gca().transAxes)
    plt.text(0.1, 0.3, f"Significant: {'Yes' if results['statistical_test']['significant'] else 'No'}", 
             fontsize=11, transform=plt.gca().transAxes, 
             color='green' if results['statistical_test']['significant'] else 'red', weight='bold')
    plt.axis('off')
    plt.title('Statistical Summary')
    
    plt.tight_layout()
    plt.savefig(f"{base_path}/results/statistical_validation_2d/fair_statistical_comparison.png", 
                dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"å…¬å¹³å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {base_path}/results/statistical_validation_2d/")

# ================================
# ä¸»æ‰§è¡Œ
# ================================

if __name__ == "__main__":
    print("ğŸš€ å…¬å¹³çš„FNO-RC 2D Navier-Stokesç»Ÿè®¡éªŒè¯")
    print("ğŸ“Š ä¸¥æ ¼ç¡®ä¿ä¸¤ä¸ªæ¨¡å‹ä½¿ç”¨å®Œå…¨ç›¸åŒçš„å‚æ•°")
    print("â±ï¸  ç»Ÿä¸€è®­ç»ƒepochs: 100")
    print("ğŸ“ æ•°æ®è·¯å¾„: /content/drive/MyDrive/ns_data_N600_clean.pt")
    print("ğŸ• é¢„è®¡è¿è¡Œæ—¶é—´: 2-3å°æ—¶")
    print()
    
    results = run_fair_statistical_experiments()
    
    print("\nğŸ‰ å…¬å¹³ç»Ÿè®¡éªŒè¯å®Œæˆï¼")
    print("âœ… ä½¿ç”¨äº†å®Œå…¨ç›¸åŒçš„è¶…å‚æ•°å’Œè®­ç»ƒæµç¨‹")
    print("ğŸ“ˆ ç»“æœå…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰")
    print("ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°Google Drive")
