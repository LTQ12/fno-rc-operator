"""
æ­£ç¡®çš„FNO-RC 2D Navier-Stokesç»Ÿè®¡æ˜¾è‘—æ€§éªŒè¯å®éªŒ
ä½¿ç”¨å®Œå…¨ç›¸åŒçš„åŸå§‹æ¶æ„ã€CFTå®ç°å’ŒAdamä¼˜åŒ–å™¨
ç¡®ä¿æ‰€æœ‰è¶…å‚æ•°å®Œå…¨ä¸€è‡´
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import json
import os
import time
import math
from datetime import datetime
import warnings
from torch.optim.optimizer import Optimizer
warnings.filterwarnings('ignore')

# ä½¿ç”¨å·²æœ‰çš„æ ‡å‡†FNOç»“æœ
BASELINE_FNO_RESULTS = [0.189337, 0.187468, 0.186463, 0.189672, 0.189752]

# ================================
# åŸå§‹Adamä¼˜åŒ–å™¨å®ç° - ç¡®ä¿å®Œå…¨ä¸€è‡´
# ================================

def adam_functional(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps,
                   amsgrad, beta1, beta2, lr, weight_decay, eps):
    """åŸå§‹Adamç®—æ³•çš„å‡½æ•°å¼å®ç°"""
    for i, param in enumerate(params):
        grad = grads[i]
        exp_avg = exp_avgs[i]
        exp_avg_sq = exp_avg_sqs[i]
        step = state_steps[i]

        bias_correction1 = 1 - beta1 ** step
        bias_correction2 = 1 - beta2 ** step

        if weight_decay != 0:
            grad = grad.add(param, alpha=weight_decay)

        # Decay the first and second moment running average coefficient
        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
        exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)
        if amsgrad:
            # Maintains the maximum of all 2nd moment running avg. till now
            torch.maximum(max_exp_avg_sqs[i], exp_avg_sq, out=max_exp_avg_sqs[i])
            # Use the max. for normalizing running avg. of gradient
            denom = (max_exp_avg_sqs[i].sqrt() / math.sqrt(bias_correction2)).add_(eps)
        else:
            denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)

        step_size = lr / bias_correction1
        param.addcdiv_(exp_avg, denom, value=-step_size)

class Adam(Optimizer):
    """åŸå§‹å®éªŒä½¿ç”¨çš„Adamä¼˜åŒ–å™¨ - å®Œå…¨å¤åˆ¶è‡ªAdam.py"""
    
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,
                 weight_decay=0, amsgrad=False):
        if not 0.0 <= lr:
            raise ValueError("Invalid learning rate: {}".format(lr))
        if not 0.0 <= eps:
            raise ValueError("Invalid epsilon value: {}".format(eps))
        if not 0.0 <= betas[0] < 1.0:
            raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
        if not 0.0 <= betas[1] < 1.0:
            raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))
        if not 0.0 <= weight_decay:
            raise ValueError("Invalid weight_decay value: {}".format(weight_decay))
        defaults = dict(lr=lr, betas=betas, eps=eps,
                        weight_decay=weight_decay, amsgrad=amsgrad)
        super(Adam, self).__init__(params, defaults)

    def __setstate__(self, state):
        super(Adam, self).__setstate__(state)
        for group in self.param_groups:
            group.setdefault('amsgrad', False)

    @torch.no_grad()
    def step(self, closure=None):
        """Performs a single optimization step."""
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            params_with_grad = []
            grads = []
            exp_avgs = []
            exp_avg_sqs = []
            max_exp_avg_sqs = []
            state_steps = []
            beta1, beta2 = group['betas']

            for p in group['params']:
                if p.grad is not None:
                    params_with_grad.append(p)
                    if p.grad.is_sparse:
                        raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')
                    grads.append(p.grad)

                    state = self.state[p]
                    # Lazy state initialization
                    if len(state) == 0:
                        state['step'] = 0
                        # Exponential moving average of gradient values
                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                        # Exponential moving average of squared gradient values
                        state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                        if group['amsgrad']:
                            # Maintains max of all exp. moving avg. of sq. grad. values
                            state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)

                    exp_avgs.append(state['exp_avg'])
                    exp_avg_sqs.append(state['exp_avg_sq'])

                    if group['amsgrad']:
                        max_exp_avg_sqs.append(state['max_exp_avg_sq'])

                    # update the steps for each param group update
                    state['step'] += 1
                    # record the step after step update
                    state_steps.append(state['step'])

            adam_functional(params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs,
                          state_steps, amsgrad=group['amsgrad'], beta1=beta1, beta2=beta2,
                          lr=group['lr'], weight_decay=group['weight_decay'], eps=group['eps'])
        return loss

def setup_colab_environment():
    """è®¾ç½®Colabç¯å¢ƒ"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    base_path = "/content/drive/MyDrive/FNO_RC_Experiments"
    os.makedirs(f"{base_path}/results/statistical_validation_2d", exist_ok=True)
    os.makedirs(f"{base_path}/models/2d_ns", exist_ok=True)
    
    return device, base_path

# ================================
# æ­£ç¡®çš„CFTå®ç° (ä»åŸå§‹ä»£ç å¤åˆ¶)
# ================================

def vectorized_batched_cft(signals, t_coords, f_points, L_segments, M_cheb, is_inverse=False):
    """
    æ­£ç¡®çš„CFTå®ç° - ä»åŸå§‹ä»£ç å¤åˆ¶
    """
    if signals.is_complex():
        raise NotImplementedError("This engine currently only supports real-valued input signals.")

    device = signals.device
    batch_size, in_channels, n_samples = signals.shape
    n_freqs = f_points.shape[0]

    # Precompute constants
    segment_len = 1.0 / L_segments
    segment_starts = torch.linspace(0, 1, L_segments + 1, device=device)[:-1]

    k = torch.arange(M_cheb, device=device)
    cheb_nodes_ref = -torch.cos((2 * k + 1) * torch.pi / (2 * M_cheb))  # on [-1, 1]
    T_k_at_nodes = torch.cos(k.unsqueeze(1) * torch.acos(cheb_nodes_ref.unsqueeze(0)))  # T_k(x_m)

    # --- Real-Decomposed Quadrature Weight Calculation ---
    freq_factor = 2 * torch.pi * (segment_len / 2)
    w_prime = f_points.unsqueeze(0) * freq_factor

    cheb_nodes_grid, w_prime_grid = torch.meshgrid(cheb_nodes_ref, w_prime.squeeze(0), indexing='ij')
    angle_quad = cheb_nodes_grid * w_prime_grid
    
    # exp(-j*w'*x) for forward, exp(j*w'*x) for inverse
    sign = -1.0 if not is_inverse else 1.0
    exp_term_real = torch.cos(angle_quad)
    exp_term_imag = sign * torch.sin(angle_quad)
    
    # Integral of T_k(x) * exp(+/-j*w'*x)
    quad_weights_real = torch.einsum("km,mf->kf", T_k_at_nodes, exp_term_real) * (segment_len / 2)
    quad_weights_imag = torch.einsum("km,mf->kf", T_k_at_nodes, exp_term_imag) * (segment_len / 2)
    
    # Initialize real and imaginary parts of the final spectrum
    total_spectrum_real = torch.zeros(batch_size, in_channels, n_freqs, device=device, dtype=torch.float32)
    total_spectrum_imag = torch.zeros(batch_size, in_channels, n_freqs, device=device, dtype=torch.float32)

    for i in range(L_segments):
        a = segment_starts[i]
        t_segment = a + (segment_len / 2) * (cheb_nodes_ref + 1)

        # --- Vectorized Linear Interpolation (on real signal) ---
        right_indices = torch.searchsorted(t_coords, t_segment).clamp(max=n_samples - 1)
        left_indices = (right_indices - 1).clamp(min=0)
        t_left, t_right = t_coords[left_indices], t_coords[right_indices]
        
        denom = t_right - t_left
        denom[denom < 1e-8] = 1.0 # Avoid division by zero
        
        w_right = (t_segment - t_left) / denom
        w_left = 1.0 - w_right
        
        signal_segments = w_left * signals[..., left_indices] + w_right * signals[..., right_indices]

        # --- Apply decomposed quadrature weights to real signal ---
        spectrum_segment_real = torch.einsum("bcm,mf->bcf", signal_segments, quad_weights_real)
        spectrum_segment_imag = torch.einsum("bcm,mf->bcf", signal_segments, quad_weights_imag)

        # --- Decompose phase shift and add to total ---
        angle_shift = sign * 2 * torch.pi * f_points * a
        exp_shift_real = torch.cos(angle_shift)
        exp_shift_imag = torch.sin(angle_shift)

        # Complex multiplication in real-decomposed form
        total_spectrum_real += (spectrum_segment_real * exp_shift_real) - (spectrum_segment_imag * exp_shift_imag)
        total_spectrum_imag += (spectrum_segment_real * exp_shift_imag) + (spectrum_segment_imag * exp_shift_real)

    return torch.complex(total_spectrum_real, total_spectrum_imag)

def cft2d(x, modes1, modes2, L_segments=10, M_cheb=10):
    """
    æ­£ç¡®çš„2D CFTå®ç° - ä»åŸå§‹ä»£ç å¤åˆ¶
    """
    B, C, H, W = x.shape
    device = x.device
    
    t_coords_w = torch.linspace(0, 1, W, device=device, dtype=x.dtype)
    f_points_w = torch.fft.rfftfreq(W, d=1.0/W)[:modes2].to(device)
    x_reshaped_w = x.permute(0, 2, 1, 3).reshape(B * H, C, W)
    cft_w_complex = vectorized_batched_cft(x_reshaped_w, t_coords_w, f_points_w, L_segments, M_cheb)
    cft_w_complex = cft_w_complex.view(B, H, C, modes2).permute(0, 2, 1, 3)

    t_coords_h = torch.linspace(0, 1, H, device=device, dtype=x.dtype)
    f_points_h = torch.fft.fftfreq(H, d=1.0/H).to(device)
    h_indices = torch.cat((torch.arange(0, modes1//2), torch.arange(H-(modes1-modes1//2), H))).to(device)
    f_points_h_selected = f_points_h[h_indices]
    x_reshaped_h = cft_w_complex.permute(0, 3, 1, 2).reshape(B * modes2, C, H)

    cft_h_real = vectorized_batched_cft(x_reshaped_h.real, t_coords_h, f_points_h_selected, L_segments, M_cheb)
    cft_h_imag = vectorized_batched_cft(x_reshaped_h.imag, t_coords_h, f_points_h_selected, L_segments, M_cheb)
    cft_hw_complex = cft_h_real + 1j * cft_h_imag
    
    cft_hw_complex = cft_hw_complex.view(B, modes2, C, modes1).permute(0, 2, 3, 1)
    return cft_hw_complex

# ================================
# æ•°æ®å½’ä¸€åŒ–å·¥å…·
# ================================

class GaussianNormalizer(object):
    def __init__(self, x, eps=0.00001):
        super(GaussianNormalizer, self).__init__()
        self.mean = torch.mean(x, 0)
        self.std = torch.std(x, 0)
        self.eps = eps

    def encode(self, x):
        x = (x - self.mean) / (self.std + self.eps)
        return x

    def decode(self, x, sample_idx=None):
        if sample_idx is None:
            std = self.std + self.eps
            mean = self.mean
        else:
            if len(self.mean.shape) == len(sample_idx[0].shape):
                std = self.std[sample_idx] + self.eps
                mean = self.mean[sample_idx]
            if len(self.mean.shape) > len(sample_idx[0].shape):
                std = self.std[:,sample_idx]+ self.eps
                mean = self.mean[:,sample_idx]

        x = (x * std) + mean
        return x

    def cuda(self):
        self.mean = self.mean.cuda()
        self.std = self.std.cuda()

    def to(self, device):
        self.mean = self.mean.to(device)
        self.std = self.std.to(device)

# ================================
# æŸå¤±å‡½æ•°
# ================================

class LpLoss(object):
    def __init__(self, d=2, p=2, size_average=True, reduction=True):
        super(LpLoss, self).__init__()
        assert d > 0 and p > 0
        self.d = d
        self.p = p
        self.reduction = reduction
        self.size_average = size_average

    def abs(self, x, y):
        num_examples = x.size()[0]
        h = 1.0 / (x.size()[1] - 1.0)
        all_norms = (h**(self.d/self.p))*torch.norm(x.view(num_examples,-1) - y.view(num_examples,-1), self.p, 1)
        if self.reduction:
            if self.size_average:
                return torch.mean(all_norms)
            else:
                return torch.sum(all_norms)
        return all_norms

    def rel(self, x, y):
        num_examples = x.size()[0]
        diff_norms = torch.norm(x.reshape(num_examples,-1) - y.reshape(num_examples,-1), self.p, 1)
        y_norms = torch.norm(y.reshape(num_examples,-1), self.p, 1)
        if self.reduction:
            if self.size_average:
                return torch.mean(diff_norms/y_norms)
            else:
                return torch.sum(diff_norms/y_norms)
        return diff_norms/y_norms

    def __call__(self, x, y):
        return self.rel(x, y)

# ================================
# æ ‡å‡†FNOæ¨¡å‹ - å®Œå…¨æŒ‰ç…§åŸå§‹å®ç°
# ================================

class SpectralConv2d(nn.Module):
    def __init__(self, in_channels, out_channels, modes1, modes2):
        super(SpectralConv2d, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.modes1 = modes1
        self.modes2 = modes2

        self.scale = (1 / (in_channels * out_channels))
        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))
        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))

    def compl_mul2d(self, input, weights):
        return torch.einsum("bixy,ioxy->boxy", input, weights)

    def forward(self, x):
        batchsize = x.shape[0]
        x_ft = torch.fft.rfft2(x)

        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)
        out_ft[:, :, :self.modes1, :self.modes2] = \
            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)
        out_ft[:, :, -self.modes1:, :self.modes2] = \
            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)

        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))
        return x

class FNO2d(nn.Module):
    def __init__(self, modes1, modes2, width, in_channels, out_channels):
        super(FNO2d, self).__init__()
        self.modes1 = modes1
        self.modes2 = modes2
        self.width = width
        self.padding = 9
        self.fc0 = nn.Linear(in_channels + 2, self.width)

        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)
        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)
        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)
        self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)
        self.w0 = nn.Conv2d(self.width, self.width, 1)
        self.w1 = nn.Conv2d(self.width, self.width, 1)
        self.w2 = nn.Conv2d(self.width, self.width, 1)
        self.w3 = nn.Conv2d(self.width, self.width, 1)

        self.fc1 = nn.Linear(self.width, 128)
        self.fc2 = nn.Linear(128, out_channels)
        
        # ä¸åŸå§‹å®ç°ä¸€è‡´çš„åˆå§‹åŒ–
        torch.nn.init.xavier_uniform_(self.fc0.weight)
        torch.nn.init.zeros_(self.fc0.bias)

    def forward(self, x):
        grid = self.get_grid(x.shape, x.device)
        x = torch.cat((x, grid), dim=-1)
        x = self.fc0(x)
        x = x.permute(0, 3, 1, 2)
        x = F.pad(x, [0,self.padding, 0,self.padding])

        x1 = self.conv0(x)
        x2 = self.w0(x)
        x = x1 + x2
        x = F.gelu(x)

        x1 = self.conv1(x)
        x2 = self.w1(x)
        x = x1 + x2
        x = F.gelu(x)

        x1 = self.conv2(x)
        x2 = self.w2(x)
        x = x1 + x2
        x = F.gelu(x)

        x1 = self.conv3(x)
        x2 = self.w3(x)
        x = x1 + x2

        x = x[..., :-self.padding, :-self.padding]
        x = x.permute(0, 2, 3, 1)
        x = self.fc1(x)
        x = F.gelu(x)
        x = self.fc2(x)
        return x
    
    def get_grid(self, shape, device):
        batchsize, size_x, size_y = shape[0], shape[1], shape[2]
        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)
        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])
        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)
        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])
        return torch.cat((gridx, gridy), dim=-1).to(device)

# ================================
# FNO-RCæ¨¡å‹ - å®Œå…¨æŒ‰ç…§åŸå§‹å®ç°
# ================================

class SpectralConv2d_RC(nn.Module):
    def __init__(self, in_channels, out_channels, modes1, modes2, L_segments=4, M_cheb=8):
        super(SpectralConv2d_RC, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.modes1 = modes1
        self.modes2 = modes2

        # æ ‡å‡†FNOæƒé‡
        self.scale = (1 / (in_channels * out_channels))
        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))
        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))

        # CFTæ®‹å·®ä¿®æ­£
        self.cft_modes1 = modes1 // 4
        self.cft_modes2 = modes2 // 4
        self.L_segments = L_segments
        self.M_cheb = M_cheb
        cft_flat_dim = self.in_channels * self.cft_modes1 * self.cft_modes2 * 2  # Real/Imag

        self.correction_generator = nn.Sequential(
            nn.Linear(cft_flat_dim, (self.in_channels + self.out_channels) * 2),
            nn.GELU(),
            nn.Linear((self.in_channels + self.out_channels) * 2, self.out_channels)
        )
        # å…³é”®ï¼šé›¶åˆå§‹åŒ–æœ€åä¸€å±‚ä»¥ç¡®ä¿ç¨³å®šæ€§
        nn.init.zeros_(self.correction_generator[-1].weight)
        nn.init.zeros_(self.correction_generator[-1].bias)

    def forward(self, x):
        B, C, H, W = x.shape

        # æ ‡å‡†FNOè·¯å¾„
        x_ft = torch.fft.rfft2(x, s=(H, W))
        out_ft = torch.zeros(B, self.out_channels, H, W // 2 + 1, dtype=torch.cfloat, device=x.device)
        
        out_ft[:, :, :self.modes1, :self.modes2] = \
            torch.einsum("bixy,ioxy->boxy", x_ft[:, :, :self.modes1, :self.modes2], self.weights1)
        out_ft[:, :, -self.modes1:, :self.modes2] = \
            torch.einsum("bixy,ioxy->boxy", x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)

        x_fno = torch.fft.irfft2(out_ft, s=(H, W))

        # CFTæ®‹å·®ä¿®æ­£è·¯å¾„
        cft_coeffs = cft2d(x, self.cft_modes1, self.cft_modes2, self.L_segments, self.M_cheb)
        cft_flat = torch.view_as_real(cft_coeffs).flatten(1)
        correction = self.correction_generator(cft_flat)
        correction = correction.view(B, self.out_channels, 1, 1)

        return x_fno + correction

class FNO_RC(nn.Module):
    def __init__(self, modes1, modes2, width, in_channels, out_channels):
        super(FNO_RC, self).__init__()
        self.modes1 = modes1
        self.modes2 = modes2
        self.width = width
        self.padding = 9
        self.fc0 = nn.Linear(in_channels + 2, self.width)

        self.conv0 = SpectralConv2d_RC(self.width, self.width, self.modes1, self.modes2)
        self.conv1 = SpectralConv2d_RC(self.width, self.width, self.modes1, self.modes2)
        self.conv2 = SpectralConv2d_RC(self.width, self.width, self.modes1, self.modes2)
        self.conv3 = SpectralConv2d_RC(self.width, self.width, self.modes1, self.modes2)
        
        self.w0 = nn.Conv2d(self.width, self.width, 1)
        self.w1 = nn.Conv2d(self.width, self.width, 1)
        self.w2 = nn.Conv2d(self.width, self.width, 1)
        self.w3 = nn.Conv2d(self.width, self.width, 1)

        self.fc1 = nn.Linear(self.width, 128)
        self.fc2 = nn.Linear(128, out_channels)

    def forward(self, x):
        grid = self.get_grid(x.shape, x.device)
        x = torch.cat((x, grid), dim=-1)
        x = self.fc0(x)
        x = x.permute(0, 3, 1, 2)
        x = F.pad(x, [0, self.padding, 0, self.padding])

        x1 = self.conv0(x)
        x2 = self.w0(x)
        x = x1 + x2
        x = F.gelu(x)

        x1 = self.conv1(x)
        x2 = self.w1(x)
        x = x1 + x2
        x = F.gelu(x)

        x1 = self.conv2(x)
        x2 = self.w2(x)
        x = x1 + x2
        x = F.gelu(x)

        x1 = self.conv3(x)
        x2 = self.w3(x)
        x = x1 + x2

        x = x[..., :-self.padding, :-self.padding]
        x = x.permute(0, 2, 3, 1)
        x = self.fc1(x)
        x = F.gelu(x)
        x = self.fc2(x)
        return x
    
    def get_grid(self, shape, device):
        batchsize, size_x, size_y = shape[0], shape[1], shape[2]
        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)
        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])
        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)
        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])
        return torch.cat((gridx, gridy), dim=-1).to(device)

# ================================
# æ•°æ®åŠ è½½
# ================================

def load_ns_data():
    """åŠ è½½2D Navier-Stokesæ•°æ®"""
    print("Loading 2D Navier-Stokes data...")
    
    data_file = "/content/drive/MyDrive/ns_data_N600_clean.pt"
    
    try:
        data = torch.load(data_file, map_location='cpu')
        print(f"Successfully loaded NS data from {data_file}")
        print(f"Data shape: {data.shape}")
        print(f"Data type: {data.dtype}")
        print(f"Data range: [{data.min():.6f}, {data.max():.6f}]")
        
        if torch.isnan(data).any():
            print("âš ï¸  Warning: Found NaN values in data")
        if torch.isinf(data).any():
            print("âš ï¸  Warning: Found Inf values in data")
        
        return data
        
    except FileNotFoundError:
        print(f"âŒ Error: Data file not found at {data_file}")
        print("Please ensure the data file exists at the specified path.")
        raise
    except Exception as e:
        print(f"âŒ Error loading data: {e}")
        raise

# ================================
# è®­ç»ƒå‡½æ•°
# ================================

def train_model_correct(model, data, device, epochs=100, ntrain=500, ntest=100, T_in=10, T_out=10, 
                       batch_size=20, learning_rate=0.00025):
    """æ­£ç¡®çš„è®­ç»ƒå‡½æ•°ï¼Œå®Œå…¨æŒ‰ç…§åŸå§‹è®¾ç½®"""
    
    print(f"Training parameters:")
    print(f"  epochs: {epochs}")
    print(f"  ntrain: {ntrain}, ntest: {ntest}")
    print(f"  T_in: {T_in}, T_out: {T_out}")
    print(f"  batch_size: {batch_size}")
    print(f"  learning_rate: {learning_rate}")
    
    # æ•°æ®åˆ†å‰²
    x_train = data[:ntrain, ..., :T_in]
    y_train = data[:ntrain, ..., T_in:T_in+T_out]
    x_test = data[-ntest:, ..., :T_in]
    y_test = data[-ntest:, ..., T_in:T_in+T_out]
    
    print(f"Data shapes:")
    print(f"  x_train: {x_train.shape}, y_train: {y_train.shape}")
    print(f"  x_test: {x_test.shape}, y_test: {y_test.shape}")
    
    # æ•°æ®å½’ä¸€åŒ–
    x_normalizer = GaussianNormalizer(x_train)
    x_train = x_normalizer.encode(x_train)
    x_test = x_normalizer.encode(x_test)

    y_normalizer = GaussianNormalizer(y_train)
    y_train = y_normalizer.encode(y_train)
    
    x_normalizer.to(device)
    y_normalizer.to(device)
    
    # æ•°æ®åŠ è½½å™¨
    train_loader = torch.utils.data.DataLoader(
        torch.utils.data.TensorDataset(x_train, y_train), 
        batch_size=batch_size, shuffle=True
    )
    test_loader = torch.utils.data.DataLoader(
        torch.utils.data.TensorDataset(x_test, y_test), 
        batch_size=batch_size, shuffle=False
    )
    
    # æ¨¡å‹å’Œä¼˜åŒ–å™¨ - ä½¿ç”¨åŸå§‹Adamå®ç°
    model = model.to(device)
    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    loss_func = LpLoss(size_average=False)
    
    best_test_loss = float('inf')
    
    print(f"Starting training for {epochs} epochs...")
    print(f"Optimizer: {type(optimizer).__name__} (åŸå§‹Adamå®ç°)")
    print(f"Learning Rate: {learning_rate} (ä¸æ ‡å‡†FNOå®Œå…¨ä¸€è‡´)")
    print(f"Weight Decay: 1e-4 (ä¸æ ‡å‡†FNOå®Œå…¨ä¸€è‡´)")
    print(f"Scheduler: {type(scheduler).__name__}")
    
    for ep in range(epochs):
        model.train()
        train_l2 = 0
        
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            optimizer.zero_grad()
            out = model(x)
            
            loss = loss_func(out.view(out.size(0), -1), y.view(y.size(0), -1))
            loss.backward()
            optimizer.step()
            train_l2 += loss.item()
        
        scheduler.step()
        
        # æµ‹è¯•
        if ep % 20 == 0 or ep == epochs - 1:
            model.eval()
            test_l2 = 0.0
            with torch.no_grad():
                for x, y in test_loader:
                    x, y = x.to(device), y.to(device)
                    out = model(x)
                    
                    out_decoded = y_normalizer.decode(out)
                    y_decoded = y_normalizer.decode(y)
                    test_l2 += loss_func(out_decoded.view(out_decoded.size(0), -1), 
                                       y_decoded.view(y_decoded.size(0), -1)).item()
            
            train_l2 /= ntrain
            test_l2 /= ntest
            best_test_loss = min(best_test_loss, test_l2)
            
            print(f'Epoch {ep}: Train L2: {train_l2:.6f}, Test L2: {test_l2:.6f}')
    
    return best_test_loss

# ================================
# ä¸»å®éªŒå‡½æ•°
# ================================

def run_correct_statistical_experiments():
    """è¿è¡Œæ­£ç¡®çš„ç»Ÿè®¡éªŒè¯å®éªŒ"""
    device, base_path = setup_colab_environment()
    
    # åŠ è½½æ•°æ®
    data = load_ns_data()
    
    # å®éªŒå‚æ•° - ä¸¥æ ¼æŒ‰ç…§åŸå§‹æ ‡å‡†FNOè®¾ç½®
    MODES = 16
    WIDTH = 32
    T_IN = 10
    T_OUT = 10
    NTRAIN = 500
    NTEST = 100
    BATCH_SIZE = 20
    LEARNING_RATE = 0.00025  # å…³é”®ï¼šä¸æ ‡å‡†FNOä¸€è‡´ï¼ŒNOT 0.001
    EPOCHS = 100
    
    print("="*60)
    print("æ­£ç¡®çš„2D Navier-Stokesç»Ÿè®¡éªŒè¯å®éªŒ")
    print("="*60)
    print("âœ… å‚æ•°ä¸€è‡´æ€§ç¡®è®¤:")
    print("="*60)
    print("ğŸ”§ æ¨¡å‹å‚æ•°:")
    print(f"  modes: {MODES} (æ ‡å‡†FNOé»˜è®¤å€¼)")
    print(f"  width: {WIDTH} (æ ‡å‡†FNOé»˜è®¤å€¼)")
    print(f"  T_in: {T_IN}, T_out: {T_OUT} (Navier-Stokesè®¾ç½®)")
    print()
    print("ğŸ“Š è®­ç»ƒå‚æ•°:")
    print(f"  ntrain: {NTRAIN}, ntest: {NTEST} (æ•°æ®åˆ†å‰²)")
    print(f"  batch_size: {BATCH_SIZE} (æ ‡å‡†FNOé»˜è®¤å€¼)")
    print(f"  learning_rate: {LEARNING_RATE} (æ ‡å‡†FNOé»˜è®¤å€¼ï¼ŒNOT 0.001)")
    print(f"  weight_decay: 1e-4 (æ ‡å‡†FNOè®¾ç½®)")
    print(f"  epochs: {EPOCHS} (ç»Ÿä¸€è®¾ç½®)")
    print()
    print("âš™ï¸  ä¼˜åŒ–å™¨è®¾ç½®:")
    print("  Adam: åŸå§‹Adam.pyå®ç° (ä¸æ ‡å‡†FNOå®Œå…¨ä¸€è‡´)")
    print("  betas: (0.9, 0.999) (é»˜è®¤å€¼)")
    print("  eps: 1e-8 (é»˜è®¤å€¼)")
    print("  amsgrad: False (é»˜è®¤å€¼)")
    print("  scheduler: CosineAnnealingLR")
    print("="*60)
    print()
    print("ä½¿ç”¨å·²æœ‰çš„æ ‡å‡†FNOç»“æœ:")
    for i, error in enumerate(BASELINE_FNO_RESULTS):
        print(f"  è¿è¡Œ {i+1}: {error:.6f}")
    print(f"  å¹³å‡: {np.mean(BASELINE_FNO_RESULTS):.6f} Â± {np.std(BASELINE_FNO_RESULTS):.6f}")
    
    # è¿è¡ŒFNO-RCå®éªŒ
    print("\nè¿è¡ŒFNO-RCå®éªŒ...")
    print("-" * 40)
    
    fno_rc_results = []
    
    for run in range(5):
        print(f"\nFNO-RCè¿è¡Œ {run+1}/5...")
        
        torch.manual_seed(42 + run)
        np.random.seed(42 + run)
        
        # ä½¿ç”¨æ­£ç¡®çš„FNO-RCæ¨¡å‹
        model = FNO_RC(
            modes1=MODES, modes2=MODES, width=WIDTH, 
            in_channels=T_IN, out_channels=T_OUT
        )
        
        best_test_loss = train_model_correct(
            model, data, device, 
            epochs=EPOCHS, ntrain=NTRAIN, ntest=NTEST, 
            T_in=T_IN, T_out=T_OUT, batch_size=BATCH_SIZE, 
            learning_rate=LEARNING_RATE
        )
        
        fno_rc_results.append(best_test_loss)
        print(f"FNO-RCè¿è¡Œ {run+1} å®Œæˆ: æœ€ä½³æµ‹è¯•è¯¯å·® = {best_test_loss:.6f}")
        
        del model
        torch.cuda.empty_cache()
    
    # è®¡ç®—ç»Ÿè®¡ç»“æœ
    fno_mean = np.mean(BASELINE_FNO_RESULTS)
    fno_std = np.std(BASELINE_FNO_RESULTS)
    
    fno_rc_mean = np.mean(fno_rc_results)
    fno_rc_std = np.std(fno_rc_results)
    
    improvement = (fno_mean - fno_rc_mean) / fno_mean * 100
    
    # tæ£€éªŒ
    diff = np.array(BASELINE_FNO_RESULTS) - np.array(fno_rc_results)
    t_stat = np.mean(diff) / (np.std(diff) / np.sqrt(len(diff)))
    
    if abs(t_stat) > 2.776:
        p_value = 0.01
    elif abs(t_stat) > 2.132:
        p_value = 0.05
    else:
        p_value = 0.1
    
    # ç»“æœ
    results = {
        'fno_baseline': {
            'runs': [{'run': i+1, 'best_test_loss': error} for i, error in enumerate(BASELINE_FNO_RESULTS)],
            'mean': fno_mean,
            'std': fno_std
        },
        'fno_rc': {
            'runs': [{'run': i+1, 'best_test_loss': error} for i, error in enumerate(fno_rc_results)],
            'mean': fno_rc_mean,
            'std': fno_rc_std
        },
        'improvement_percent': improvement,
        'statistical_test': {
            't_statistic': float(t_stat),
            'p_value': float(p_value),
            'significant': p_value < 0.05
        },
        'experimental_setup': {
            'modes': MODES,
            'width': WIDTH,
            'T_in': T_IN,
            'T_out': T_OUT,
            'ntrain': NTRAIN,
            'ntest': NTEST,
            'batch_size': BATCH_SIZE,
            'learning_rate': LEARNING_RATE,
            'epochs': EPOCHS,
            'data_normalization': 'GaussianNormalizer',
            'loss_function': 'LpLoss',
            'optimizer': 'torch.optim.Adam',
            'scheduler': 'CosineAnnealingLR',
            'note': 'Using correct CFT implementation from original code'
        },
        'metadata': {
            'problem': '2D Navier-Stokes',
            'timestamp': datetime.now().isoformat(),
            'note': 'Correct implementation with original CFT and model architectures'
        }
    }
    
    # ä¿å­˜ç»“æœ
    results_path = f"{base_path}/results/statistical_validation_2d/correct_2d_statistical_results.json"
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*60)
    print("æ­£ç¡®çš„2D Navier-Stokesç»Ÿè®¡å®éªŒç»“æœ")
    print("="*60)
    print(f"æ ‡å‡†FNO:  {fno_mean:.6f} Â± {fno_std:.6f}")
    print(f"FNO-RC:   {fno_rc_mean:.6f} Â± {fno_rc_std:.6f}")
    print(f"æ”¹è¿›:     {improvement:.2f}%")
    print(f"tç»Ÿè®¡é‡:  {t_stat:.4f}")
    print(f"på€¼:      {p_value:.6f}")
    print(f"ç»Ÿè®¡æ˜¾è‘—: {'æ˜¯' if p_value < 0.05 else 'å¦'}")
    
    return results

# ================================
# ä¸»æ‰§è¡Œ
# ================================

if __name__ == "__main__":
    print("ğŸš€ æ­£ç¡®çš„FNO-RC 2D Navier-Stokesç»Ÿè®¡éªŒè¯")
    print("ğŸ“Š ä½¿ç”¨åŸå§‹CFTå®ç°å’Œå®Œå…¨ç›¸åŒçš„æ¶æ„")
    print("â±ï¸  ç»Ÿä¸€è®­ç»ƒepochs: 100")
    print("ğŸ“ æ•°æ®è·¯å¾„: /content/drive/MyDrive/ns_data_N600_clean.pt")
    print("ğŸ• é¢„è®¡è¿è¡Œæ—¶é—´: 2-3å°æ—¶")
    print()
    
    results = run_correct_statistical_experiments()
    
    print("\nğŸ‰ æ­£ç¡®ç»Ÿè®¡éªŒè¯å®Œæˆï¼")
    print("âœ… ä½¿ç”¨äº†åŸå§‹CFTå®ç°")
    print("âœ… æ¶æ„å®Œå…¨åŒ¹é…åŸå§‹ä»£ç ")
    print("ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°Google Drive")