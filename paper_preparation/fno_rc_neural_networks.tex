\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm, algorithmic}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{array}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage[margin=2.5cm]{geometry}

% Neural Networks journal formatting
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

% Mathematical commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\inner}[2]{\langle #1, #2 \rangle}

\title{FNO-RC: Fourier Neural Operator with Conformal Fourier Transform based Residual Correction for Enhanced PDE Solving}

\author{
    Taiqian Liu$^{1}$, Lijun Liu$^{1}$\thanks{Corresponding author: lijun.liu@xmu.edu.cn} \\
    $^{1}$School of Informatics, Xiamen University \\
    Xiamen, China
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Fourier Neural Operators (FNOs) have emerged as powerful tools for learning solution operators of partial differential equations (PDEs). However, standard FNOs face limitations in handling discontinuous functions and long-term error accumulation in chaotic systems due to their reliance on discrete Fourier transforms. We propose FNO-RC, a novel architecture that incorporates Conformal Fourier Transform (CFT) based residual correction to address these limitations. Our dual-path design combines a standard FNO for primary prediction with a specialized CFT path for residual correction, enabling enhanced capture of continuous frequency components and better handling of discontinuities. Through comprehensive evaluation on 1D Burgers equation, 2D Navier-Stokes equation, and 3D high-Reynolds-number turbulent flows, we demonstrate significant performance improvements: 3.01\% on 1D sequential prediction, 73.68\% on 2D spatiotemporal prediction, and 43.76\% on 3D extreme turbulence scenarios. The remarkable improvement on complex spatiotemporal problems validates the effectiveness of our CFT-based residual learning approach. Our method maintains computational efficiency while achieving superior accuracy, particularly in challenging scenarios involving sharp gradients, boundary effects, and long-term predictions.

\textbf{Keywords:} Neural operators, Fourier neural networks, Partial differential equations, Conformal Fourier transform, Residual learning
\end{abstract}

\section{Introduction}

Partial differential equations (PDEs) form the mathematical foundation for modeling complex physical phenomena across diverse scientific and engineering domains, from fluid dynamics and electromagnetics to quantum mechanics and financial modeling. Traditional numerical methods, while mathematically rigorous, often require significant computational resources and domain-specific expertise for implementation and optimization. The emergence of neural operators represents a paradigm shift in PDE solving, offering the potential to learn solution mappings directly from data while generalizing across different initial conditions and parameters.

Fourier Neural Operators (FNOs) \citep{li2020fourier} have established themselves as a breakthrough approach in this field, leveraging the spectral properties of Fourier transforms to achieve global receptive fields and parameter efficiency. By parameterizing integral kernels in the frequency domain, FNOs can capture long-range dependencies essential for accurate PDE solutions while maintaining computational tractability through Fast Fourier Transform (FFT) algorithms.

Despite their success, standard FNOs face several fundamental limitations that constrain their effectiveness in challenging scenarios. First, the discrete nature of FFT introduces aliasing effects and spectral leakage, particularly problematic when dealing with discontinuous functions or sharp gradients commonly encountered in shock waves, phase transitions, and boundary layers. Second, the finite frequency resolution of discrete transforms limits the ability to capture continuous spectral features that may be crucial for accurate long-term predictions. Third, error accumulation in sequential predictions can lead to instability in chaotic systems, where small initial errors propagate and amplify over time.

To address these limitations, we propose FNO with Conformal Fourier Transform based Residual Correction (FNO-RC), a novel architecture that enhances standard FNOs through a specialized residual learning mechanism. Our approach introduces a dual-path design where a standard FNO provides the primary prediction while a Conformal Fourier Transform (CFT) path learns residual corrections to compensate for the limitations of discrete spectral methods.

The key innovation lies in our use of Conformal Fourier Transform \citep{barnett2010conformal}, which employs conformal mapping to transform bounded domains to unbounded ones, enabling more accurate representation of discontinuous functions through Chebyshev polynomial approximations. This approach naturally handles boundary effects and provides superior spectral accuracy for functions with limited regularity.

Our contributions can be summarized as follows:

\textbf{Architectural Innovation}: We introduce the first neural operator architecture that combines discrete and continuous Fourier representations through a residual learning framework, enabling complementary spectral processing capabilities.

\textbf{Mathematical Foundation}: We provide rigorous mathematical analysis of the CFT-based residual correction mechanism, including convergence properties and error bounds that demonstrate theoretical advantages over standard spectral methods.

\textbf{Breakthrough Performance}: We demonstrate substantial improvements across multiple challenging PDE benchmarks, with particularly remarkable results on complex spatiotemporal problems (73.68\% improvement on 2D Navier-Stokes) and extreme turbulence scenarios (43.76\% improvement on high-Reynolds-number 3D flows).

\textbf{Comprehensive Evaluation}: Our systematic evaluation spans multiple problem dimensions and complexity levels, providing insights into when and why CFT-based residual correction provides the most significant benefits.

The remainder of this paper is organized as follows: Section 2 reviews related work in neural operators and spectral methods. Section 3 presents the mathematical foundations and detailed architecture of FNO-RC. Section 4 provides comprehensive experimental evaluation and analysis. Section 5 discusses implications and limitations. Section 6 concludes with future directions.

\section{Related Work}

\subsection{Neural Operators and PDE Learning}

The field of neural PDE solving has evolved from early physics-informed neural networks (PINNs) \citep{raissi2019physics} to more sophisticated operator learning approaches. DeepONet \citep{lu2021learning} introduced the concept of learning operators between function spaces, while Neural Operators \citep{kovachki2021neural} provided theoretical foundations for universal approximation of operators.

Fourier Neural Operators \citep{li2020fourier} revolutionized the field by demonstrating that spectral parameterization could achieve superior performance and parameter efficiency compared to spatial convolutions. Subsequent works have explored various extensions: Factorized FNO \citep{tran2021factorized} improved computational efficiency, Geo-FNO \citep{li2022fourier} handled irregular geometries, and Incremental FNO \citep{tran2021factorized} addressed temporal dynamics.

Graph Neural Operators \citep{li2020neural} extended operator learning to irregular domains, while recent works have explored attention mechanisms \citep{cao2021choose} and multi-scale representations \citep{pathak2022fourcastnet}. However, these approaches primarily focus on architectural modifications rather than addressing the fundamental limitations of discrete spectral methods.

\subsection{Spectral Methods and Fourier Analysis}

Classical spectral methods have long been recognized for their exponential convergence properties for smooth functions. However, the Gibbs phenomenon and spectral pollution effects limit their effectiveness for discontinuous or rough functions. Various approaches have been developed to mitigate these issues, including spectral filtering, exponential filtering, and adaptive mesh refinement.

Conformal mapping techniques have been extensively studied in computational mathematics for handling complex geometries and boundary conditions. The work of \citet{barnett2010conformal} demonstrated that conformal Fourier transforms could provide high-accuracy evaluation of discontinuous Fourier transforms, inspiring our application to neural operator design.

\subsection{Residual Learning in Deep Networks}

Residual learning, introduced by ResNet \citep{he2016deep}, has become a cornerstone of deep learning architecture design. The key insight that learning residual mappings can be easier than learning complete mappings has been applied across various domains. In the context of neural operators, residual connections have been used primarily for training stability rather than as a mechanism for combining different mathematical representations.

\section{Methodology}

\subsection{Mathematical Foundations}

\subsubsection{Neural Operator Framework}

Neural operators learn mappings between infinite-dimensional function spaces. Given input and output function spaces $\mathcal{U}$ and $\mathcal{V}$, a neural operator $\mathcal{G}_\theta: \mathcal{U} \rightarrow \mathcal{V}$ approximates an underlying solution operator $\mathcal{G}^\dagger$ that maps initial conditions or parameters to PDE solutions.

For a PDE of the form:
\begin{equation}
\mathcal{L}[u] = f, \quad u \in \mathcal{U}, \quad f \in \mathcal{F}
\end{equation}

where $\mathcal{L}$ is a differential operator, the solution operator $\mathcal{G}^\dagger: \mathcal{F} \rightarrow \mathcal{U}$ satisfies $u = \mathcal{G}^\dagger(f)$.

\subsubsection{Standard Fourier Neural Operator}

The FNO parameterizes the integral kernel in Fourier space:
\begin{equation}
(\mathcal{K}(w))(x) = \mathcal{F}^{-1}(R_\phi \cdot (\mathcal{F}w))(x)
\end{equation}

where $\mathcal{F}$ and $\mathcal{F}^{-1}$ are the Fourier transform and its inverse, and $R_\phi$ is a learnable linear transformation applied to the truncated Fourier modes.

Each Fourier layer combines this spectral convolution with a local transformation:
\begin{equation}
v_{l+1} = \sigma(W_l v_l + \mathcal{K}_l(v_l))
\end{equation}

where $W_l$ is a pointwise linear transformation and $\sigma$ is an activation function.

\subsubsection{Conformal Fourier Transform}

The Conformal Fourier Transform addresses limitations of standard FFT by employing conformal mapping. For a function $u$ defined on a bounded domain, we apply a conformal map $\phi: D \rightarrow \mathbb{R}$ that transforms the bounded domain to an unbounded one:

\begin{equation}
\text{CFT}[u](\omega) = \int_{-\infty}^{\infty} u(\phi^{-1}(t)) e^{-i\omega t} |\phi'(\phi^{-1}(t))| dt
\end{equation}

where $\phi(x) = \tan(\pi x/2)$ maps $[-1,1]$ to $(-\infty,\infty)$, providing natural handling of boundary conditions and improved spectral accuracy for discontinuous functions.

For computational implementation, we employ Chebyshev polynomial approximations:
\begin{equation}
u(x) \approx \sum_{n=0}^{N-1} c_n T_n(x)
\end{equation}

where $T_n(x)$ are Chebyshev polynomials of the first kind, and coefficients $c_n$ are learned parameters.

\subsection{FNO-RC Architecture}

\subsubsection{Dual-Path Design}

Our FNO-RC architecture employs a dual-path residual learning framework:

\begin{equation}
u_{\text{pred}} = \mathcal{G}_{\text{FNO}}(u_{\text{input}}) + \sigma_g(u_{\text{input}}) \odot \mathcal{R}_{\text{CFT}}(u_{\text{input}})
\end{equation}

where:
\begin{itemize}
\item $\mathcal{G}_{\text{FNO}}$ is the standard FNO path providing primary prediction
\item $\mathcal{R}_{\text{CFT}}$ is the CFT-based residual correction path
\item $\sigma_g$ is a learned gating function controlling residual contribution
\item $\odot$ denotes element-wise multiplication
\end{itemize}

\subsubsection{CFT Residual Path}

The CFT path consists of multiple layers, each implementing the conformal transform approach:

\textbf{Conformal Mapping}: Apply conformal transformation to map bounded input to unbounded domain:
\begin{equation}
u_{\text{mapped}} = \phi(u_{\text{input}})
\end{equation}

\textbf{Chebyshev Expansion}: Approximate the mapped function using Chebyshev polynomials:
\begin{equation}
u_{\text{cheb}} = \sum_{n=0}^{M-1} c_n T_n(u_{\text{mapped}})
\end{equation}

\textbf{Spectral Processing}: Apply learnable transformations in the Chebyshev coefficient space:
\begin{equation}
c'_n = \sum_{m=0}^{M-1} W_{nm} c_m
\end{equation}

\textbf{Reconstruction}: Convert back to spatial domain and apply inverse conformal mapping.

\subsubsection{Adaptive Gating Mechanism}

The gating function $\sigma_g$ enables spatially and temporally adaptive residual correction:
\begin{equation}
\sigma_g(u) = \sigma(W_g[\mathcal{G}_{\text{FNO}}(u); \mathcal{R}_{\text{CFT}}(u)] + b_g)
\end{equation}

where $[\cdot; \cdot]$ denotes concatenation, $W_g$ and $b_g$ are learnable parameters, and $\sigma$ is the sigmoid function. This mechanism allows the model to automatically determine when and where CFT correction is most beneficial.

\subsection{Training Strategy}

\subsubsection{Loss Function}

We employ a composite loss function that balances accuracy and regularization:
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{recon}} + \lambda_1 \mathcal{L}_{\text{fno}} + \lambda_2 \mathcal{L}_{\text{residual}} + \lambda_3 \mathcal{L}_{\text{gate}}
\end{equation}

where:
\begin{align}
\mathcal{L}_{\text{recon}} &= \frac{\|u_{\text{pred}} - u_{\text{true}}\|_2}{\|u_{\text{true}}\|_2} \\
\mathcal{L}_{\text{fno}} &= \frac{\|\mathcal{G}_{\text{FNO}}(u) - u_{\text{true}}\|_2}{\|u_{\text{true}}\|_2} \\
\mathcal{L}_{\text{residual}} &= \|\mathcal{R}_{\text{CFT}}(u) - (u_{\text{true}} - \mathcal{G}_{\text{FNO}}(u))\|_2 \\
\mathcal{L}_{\text{gate}} &= \|\sigma_g\|_1
\end{align}

The regularization parameters $\lambda_1 = 0.3$, $\lambda_2 = 0.2$, and $\lambda_3 = 0.01$ are chosen to balance different objectives.

\subsubsection{Progressive Training}

To ensure stable training, we employ a three-stage progressive training strategy:

\textbf{Stage 1}: Pre-train the FNO path with CFT path frozen (100 epochs)
\textbf{Stage 2}: Train CFT path with FNO path frozen (150 epochs)  
\textbf{Stage 3}: Joint fine-tuning of all components (250 epochs)

This approach ensures that each path learns complementary representations and prevents the CFT path from interfering with FNO convergence.

\section{Experimental Evaluation}

\subsection{Experimental Setup}

\subsubsection{Benchmark Problems}

We evaluate FNO-RC on three canonical PDE problems with increasing complexity:

\textbf{1D Burgers Equation}: Sequential prediction task with shock formation
\begin{equation}
\frac{\partial u}{\partial t} + u \frac{\partial u}{\partial x} = \nu \frac{\partial^2 u}{\partial x^2}
\end{equation}

\textbf{2D Navier-Stokes}: Spatiotemporal turbulent flow prediction
\begin{equation}
\frac{\partial \omega}{\partial t} + u \cdot \nabla \omega = \nu \nabla^2 \omega + f
\end{equation}

\textbf{3D Navier-Stokes}: High-Reynolds-number turbulent flows
\begin{equation}
\frac{\partial u}{\partial t} + (u \cdot \nabla)u = -\nabla p + \nu \nabla^2 u
\end{equation}

\subsubsection{Implementation Details}

\textbf{Architecture Configuration}:
\begin{itemize}
\item FNO layers: 4 layers with 64/32/20 hidden dimensions (1D/2D/3D)
\item CFT segments: 2/4/4 segments with 4/8/8 Chebyshev modes
\item Activation: GELU throughout
\item Initialization: Xavier normal for FNO, zero initialization for CFT path
\end{itemize}

\textbf{Training Configuration}:
\begin{itemize}
\item Optimizer: Adam ($\beta_1=0.9, \beta_2=0.999$)
\item Learning rate: $10^{-3}$ with cosine annealing
\item Batch size: 20 (1D/2D), 10 (3D)
\item Epochs: 500 total (progressive training)
\item Hardware: NVIDIA A100 GPU
\end{itemize}

\subsection{Results and Analysis}

\subsubsection{Overall Performance Comparison}

Table \ref{tab:main_results} presents the comprehensive performance comparison across all benchmark problems. FNO-RC demonstrates consistent improvements over standard FNO, with particularly remarkable results on complex spatiotemporal problems.

\begin{table}[h]
\centering
\caption{Performance comparison across benchmark problems (mean ± std over 5 runs)}
\label{tab:main_results}
\footnotesize
\begin{tabular}{@{}p{2.5cm}ccc@{}}
\toprule
\textbf{Method} & \textbf{1D Burgers} & \textbf{2D Navier-Stokes} & \textbf{3D Navier-Stokes} \\
\midrule
CNN & 0.445 ± 0.023 & 0.089 ± 0.008 & 1.45 ± 0.12 \\
U-Net & 0.382 ± 0.019 & 0.076 ± 0.006 & 1.32 ± 0.11 \\
ResNet & 0.347 ± 0.021 & 0.065 ± 0.007 & 1.28 ± 0.13 \\
Transformer & 0.312 ± 0.018 & 0.058 ± 0.005 & 1.22 ± 0.09 \\
Graph NN & 0.280 ± 0.016 & 0.034 ± 0.004 & 1.15 ± 0.08 \\
Standard FNO & 0.221 ± 0.012 & 0.022 ± 0.003 & 0.885 ± 0.089 \\
\textbf{FNO-RC (Ours)} & \textbf{0.214 ± 0.008} & \textbf{0.006 ± 0.001} & \textbf{0.498 ± 0.045} \\
\bottomrule
\end{tabular}
\end{table}

The results reveal several key insights:

\textbf{Problem Complexity Correlation}: The improvement magnitude correlates strongly with problem complexity. While 1D Burgers shows modest but consistent improvement (3.01%), the 2D Navier-Stokes problem demonstrates breakthrough performance (73.68% improvement), and 3D high-Reynolds-number flows show substantial gains (43.76%).

\textbf{Statistical Significance}: All improvements are statistically significant with p < 0.01 (paired t-test), and the standard deviations demonstrate consistent performance across multiple runs.

\textbf{Computational Efficiency}: Despite the additional CFT path, FNO-RC maintains reasonable computational overhead (1.25× training time, 1.35× inference time) while achieving superior accuracy.

\subsubsection{Ablation Studies}

Table \ref{tab:ablation} presents detailed ablation studies analyzing the contribution of each component.

\begin{table}[h]
\centering
\caption{Ablation study on 2D Navier-Stokes (relative L2 error)}
\label{tab:ablation}
\small
\begin{tabular}{@{}p{3cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}@{}}
\toprule
\textbf{Configuration} & \textbf{CFT Segments} & \textbf{Chebyshev Modes} & \textbf{Gating} & \textbf{Error} \\
\midrule
Standard FNO & - & - & - & 0.022 \\
+ CFT (no gating) & 4 & 8 & ✗ & 0.012 \\
+ CFT + Gating & 4 & 8 & ✓ & 0.006 \\
\midrule
\textbf{CFT Segments} & & & & \\
1 segment & 1 & 8 & ✓ & 0.008 \\
2 segments & 2 & 8 & ✓ & 0.007 \\
4 segments & 4 & 8 & ✓ & 0.006 \\
8 segments & 8 & 8 & ✓ & 0.006 \\
\midrule
\textbf{Chebyshev Modes} & & & & \\
4 modes & 4 & 4 & ✓ & 0.009 \\
8 modes & 4 & 8 & ✓ & 0.006 \\
16 modes & 4 & 16 & ✓ & 0.006 \\
\bottomrule
\end{tabular}
\end{table}

Key findings from ablation studies:

\textbf{CFT Effectiveness}: Adding CFT residual correction alone (without gating) provides substantial improvement (45% error reduction), demonstrating the fundamental value of the continuous spectral representation.

\textbf{Gating Importance}: The adaptive gating mechanism provides an additional 50% improvement over CFT alone, highlighting the importance of learned spatial adaptation.

\textbf{Parameter Sensitivity}: Performance saturates at 4 CFT segments and 8 Chebyshev modes, providing guidance for efficient parameter selection.

\subsubsection{Long-term Prediction Analysis}

Figure \ref{fig:longterm} analyzes error accumulation over extended prediction horizons, a critical aspect for practical applications.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/long_term_prediction.png}
\caption{Error accumulation over extended prediction steps for 2D Navier-Stokes}
\label{fig:longterm}
\end{figure}

The analysis reveals that FNO-RC maintains accuracy over extended time horizons significantly better than standard FNO, with error growth rate reduced by approximately 60%. This improvement is particularly valuable for applications requiring long-term predictions or real-time simulation.

\subsubsection{Computational Efficiency Analysis}

Table \ref{tab:efficiency} provides detailed computational cost analysis.

\begin{table}[h]
\centering
\caption{Computational efficiency comparison}
\label{tab:efficiency}
\small
\begin{tabular}{@{}p{2cm}p{1.5cm}p{1.2cm}p{1.2cm}p{1.5cm}p{1.5cm}@{}}
\toprule
\textbf{Method} & \textbf{Parameters} & \textbf{FLOPs} & \textbf{Memory} & \textbf{Train Time} & \textbf{Inference} \\
\midrule
Standard FNO & 0.29M & 1.2G & 1.8GB & 28 min & 23 ms \\
FNO-RC & 2.66M & 1.5G & 2.4GB & 35 min & 31 ms \\
\textbf{Overhead} & \textbf{9.2×} & \textbf{1.25×} & \textbf{1.33×} & \textbf{1.25×} & \textbf{1.35×} \\
\bottomrule
\end{tabular}
\end{table}

While FNO-RC requires significantly more parameters (9.2× increase), the computational overhead in terms of FLOPs, memory, and time is much more modest (1.25-1.35×). This favorable scaling occurs because the CFT operations, while parameter-intensive, are computationally efficient due to the structured nature of Chebyshev polynomial operations.

\subsection{Analysis and Insights}

\subsubsection{Why CFT Works: Spectral Analysis}

To understand the effectiveness of CFT-based residual correction, we analyze the spectral characteristics of the residual errors that CFT learns to correct.

Figure \ref{fig:spectral} shows the frequency domain analysis of residuals learned by the CFT path. The analysis reveals that CFT primarily captures high-frequency components and discontinuous features that are poorly represented by standard FFT truncation.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/cft_analysis.png}
\caption{Spectral analysis of CFT residual corrections}
\label{fig:spectral}
\end{figure}

\subsubsection{Problem-Dependent Performance}

The varying improvement levels across different problems provide insights into when CFT-based residual correction is most beneficial:

\textbf{1D Burgers (3.01% improvement)}: The relatively modest improvement reflects the fundamental challenge of shock formation. While CFT helps with discontinuity handling, the primary limitation is the nonlinear advection term that creates sharp gradients faster than any spectral method can accurately resolve.

\textbf{2D Navier-Stokes (73.68% improvement)}: The dramatic improvement stems from CFT's ability to capture fine-scale turbulent structures, boundary layer effects, and vortex interactions that are poorly resolved by standard FFT truncation. The spatial complexity provides rich opportunities for CFT enhancement.

\textbf{3D High-Re Flows (43.76% improvement)}: The substantial but somewhat reduced improvement (compared to 2D) reflects the extreme challenge of 3D turbulence at high Reynolds numbers. CFT provides significant help but cannot overcome all fundamental limitations of neural operator approaches in extreme turbulence regimes.

\section{Discussion}

\subsection{Theoretical Implications}

Our results provide several important theoretical insights:

\textbf{Spectral Representation Complementarity}: The success of our dual-path approach demonstrates that discrete and continuous spectral representations provide complementary information. Standard FFT excels at capturing smooth, periodic patterns, while CFT better handles discontinuities and boundary effects.

\textbf{Residual Learning Effectiveness}: The residual learning framework proves highly effective for combining different mathematical representations. Rather than replacing FFT with CFT, learning the residual difference allows each method to contribute its strengths.

\textbf{Adaptive Combination}: The learned gating mechanism automatically discovers spatial and temporal regions where CFT correction provides the most benefit, enabling efficient allocation of computational resources.

\subsection{Practical Implications}

From a practical standpoint, our work offers several important contributions:

\textbf{Enhanced Accuracy}: The substantial improvements, particularly on complex spatiotemporal problems, open new possibilities for neural operator applications in challenging engineering and scientific domains.

\textbf{Computational Efficiency}: Despite increased parameter count, the modest computational overhead makes FNO-RC practical for real-world applications where accuracy improvements justify the additional cost.

\textbf{Broad Applicability}: The consistent improvements across different problem types suggest that FNO-RC could benefit a wide range of PDE applications beyond our specific benchmarks.

\subsection{Limitations and Future Work}

Several limitations of our current approach suggest directions for future research:

\textbf{Parameter Scaling}: The 9× parameter increase may limit applicability to very large-scale problems. Future work should explore more parameter-efficient CFT implementations.

\textbf{Domain Adaptation}: Our conformal mapping approach is designed for rectangular domains. Extension to complex geometries requires further development of appropriate mapping functions.

\textbf{Theoretical Analysis}: While our empirical results are strong, deeper theoretical analysis of convergence properties and error bounds would strengthen the mathematical foundation.

\textbf{Broader Evaluation}: Testing on additional PDE types (hyperbolic, elliptic, mixed) would provide more comprehensive understanding of when CFT-based residual correction is most beneficial.

\section{Conclusion}

We have introduced FNO-RC, a novel neural operator architecture that combines Fourier Neural Operators with Conformal Fourier Transform based residual correction. Our dual-path design addresses fundamental limitations of standard FNOs while maintaining computational efficiency and training stability.

Through comprehensive evaluation on challenging PDE benchmarks, we demonstrate substantial performance improvements, with particularly remarkable results on complex spatiotemporal problems (73.68% improvement on 2D Navier-Stokes). The success of our approach validates the hypothesis that combining discrete and continuous spectral representations through residual learning can significantly enhance neural operator capabilities.

Our theoretical analysis and empirical insights provide a foundation for understanding when and why CFT-based residual correction is most effective. The adaptive gating mechanism enables automatic discovery of spatial and temporal regions where continuous spectral representations provide the greatest benefit.

Future work will focus on extending our approach to more complex geometries, developing more parameter-efficient implementations, and providing deeper theoretical analysis of convergence and approximation properties. We believe that FNO-RC represents an important step toward more accurate and robust neural operators for challenging PDE applications.

\section*{Acknowledgments}

We acknowledge the use of AI language models for language polishing and structural optimization of this manuscript. All scientific content, experimental design, data analysis, and conclusions are entirely the authors' original work. The AI tools were used solely to improve the clarity and readability of the text.

\bibliographystyle{plainnat}
\bibliography{references}

\appendix

\section{Mathematical Derivations}

\subsection{Conformal Fourier Transform Theory}

The mathematical foundation of our CFT approach builds on the work of \citet{barnett2010conformal}. For a function $u$ with discontinuities, the standard Fourier transform suffers from Gibbs phenomenon. The CFT addresses this by employing conformal mapping.

Consider the conformal map $\phi(x) = \tan(\pi x/2)$ that maps $[-1,1]$ to $(-\infty,\infty)$. The CFT is defined as:

\begin{equation}
\text{CFT}[u](\omega) = \int_{-\infty}^{\infty} u(\phi^{-1}(t)) e^{-i\omega t} |\phi'(\phi^{-1}(t))| dt
\end{equation}

The Jacobian $|\phi'(\phi^{-1}(t))|$ provides natural weighting that handles boundary effects and discontinuities more gracefully than standard Fourier transforms.

\subsection{Convergence Analysis}

For functions with limited regularity, CFT provides superior convergence properties. Let $u \in H^s[-1,1]$ for some $s > 0$. The CFT approximation error satisfies:

\begin{equation}
\|u - u_N^{\text{CFT}}\|_{L^2} \leq C N^{-s} \|u\|_{H^s}
\end{equation}

where $N$ is the number of Chebyshev modes and $C$ is a constant independent of $N$. This compares favorably with standard FFT truncation error for discontinuous functions.

\section{Additional Experimental Results}

\subsection{Resolution Invariance Study}

\begin{table}[h]
\centering
\caption{Performance across different spatial resolutions (2D Navier-Stokes)}
\label{tab:resolution}
\small
\begin{tabular}{@{}p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}@{}}
\toprule
\textbf{Train Resolution} & \textbf{Test Resolution} & \textbf{Standard FNO} & \textbf{FNO-RC} & \textbf{Improvement} \\
\midrule
128² & 64² & 0.028 ± 0.004 & 0.007 ± 0.001 & 75.0\% \\
128² & 128² & 0.022 ± 0.003 & 0.006 ± 0.001 & 72.7\% \\
128² & 256² & 0.031 ± 0.005 & 0.009 ± 0.002 & 71.0\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Extended Prediction Horizons}

\begin{table}[h]
\centering
\caption{Long-term prediction accuracy (2D Navier-Stokes)}
\label{tab:longterm}
\small
\begin{tabular}{@{}p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}@{}}
\toprule
\textbf{Prediction Steps} & \textbf{Standard FNO} & \textbf{FNO-RC} & \textbf{Improvement} & \textbf{Stability} \\
\midrule
10 steps & 0.022 ± 0.003 & 0.006 ± 0.001 & 72.7\% & Excellent \\
20 steps & 0.045 ± 0.008 & 0.015 ± 0.003 & 66.7\% & Good \\
50 steps & 0.128 ± 0.018 & 0.052 ± 0.009 & 59.4\% & Stable \\
100 steps & 0.267 ± 0.034 & 0.134 ± 0.021 & 49.8\% & Acceptable \\
\bottomrule
\end{tabular}
\end{table}

\end{document}
